#!/usr/bin/env python3
"""
ìë™í™”ëœ ì½˜í…ì¸  íŒŒì´í”„ë¼ì¸
Automated Content Pipeline for Longevity Knowledge Platform

ë…¼ë¬¸ ìˆ˜ì§‘ â†’ AI ìš”ì•½/í•´ì„ â†’ íŒ©íŠ¸ì²´í¬ â†’ ë°œí–‰ ì¤€ë¹„

Components:
1. Paper Discovery (PubMed, RSS feeds)
2. Document Processing (Docling)
3. Content Generation (Gemini/GPT-4)
4. Fact Checking (GPT-4)
5. Publishing Preparation
"""

import os
import re
import json
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
from dotenv import load_dotenv


def parse_json_response(text: str) -> dict:
    """Parse JSON from AI response, stripping markdown code blocks if present"""
    # Strip ```json ... ``` or ``` ... ```
    cleaned = re.sub(r'^```(?:json)?\s*\n?', '', text.strip())
    cleaned = re.sub(r'\n?```\s*$', '', cleaned)
    try:
        return json.loads(cleaned)
    except json.JSONDecodeError:
        # Try to extract JSON object from mixed text
        match = re.search(r'\{[\s\S]*\}', cleaned)
        if match:
            return json.loads(match.group())
        raise

# Load .env file
load_dotenv()

# Configuration
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
KIMI_API_KEY = os.getenv("KIMI_API_KEY")  # Moonshot AI - much cheaper alternative


@dataclass
class Paper:
    """Research paper metadata"""
    title: str
    authors: List[str]
    abstract: str
    journal: str
    doi: str
    pub_date: str
    url: str
    full_text: Optional[str] = None
    relevance_score: float = 0.0
    topics: List[str] = None


@dataclass
class ContentDraft:
    """AI-generated content draft"""
    paper: Paper
    content_type: str  # newsletter, blog, youtube_script, vod_lecture
    korean_title: str
    korean_summary: str
    korean_body: str
    english_title: str
    english_summary: str
    key_insights: List[str]
    practical_applications: List[str]
    citations: List[Dict]
    fact_check_notes: List[str]
    confidence_score: float
    created_at: str
    status: str  # draft, reviewed, published
    source: str = ""  # pubmed, biorxiv, medrxiv, clinical_trial


@dataclass
class ClinicalTrial:
    """Clinical trial metadata"""
    nct_id: str
    title: str
    status: str
    phase: str
    conditions: List[str]
    interventions: List[str]
    summary: str
    start_date: str
    url: str
    relevance_score: float = 0.0


class PaperDiscovery:
    """
    í†µí•© ë…¼ë¬¸ ë°œê²¬ ì‹œìŠ¤í…œ
    Multi-source paper discovery: PubMed, bioRxiv, medRxiv, ClinicalTrials.gov

    Integrates with MCP servers:
    - pubmed: PubMed indexed papers
    - biorxiv: bioRxiv/medRxiv preprints
    - clinical-trials: ClinicalTrials.gov
    - scholar-gateway: Academic papers
    """

    LONGEVITY_KEYWORDS = [
        # Core longevity
        "NAD+ metabolism", "senolytics", "cellular senescence",
        "mitochondrial dysfunction", "autophagy aging",
        "longevity interventions", "healthspan", "lifespan extension",
        "metabolic aging", "epigenetic clock", "telomere attrition",
        # Therapeutics
        "GLP-1 agonist aging", "rapamycin longevity", "metformin aging",
        "NMN supplementation", "resveratrol", "spermidine",
        # Emerging
        "senostatics", "SASP inhibitors", "inflammaging",
        # Cancer diagnosis & therapeutics
        "liquid biopsy cancer", "immunotherapy checkpoint",
        "CAR-T cell therapy", "cancer biomarker early detection",
        "targeted therapy oncology", "tumor microenvironment",
        # Korean research focus
        "Korean longevity", "Asian metabolic disease",
    ]

    # Broader keywords for clinical trials (more general terms work better)
    CLINICAL_TRIAL_KEYWORDS = [
        "aging", "longevity", "senolytic", "NAD+", "NMN",
        "rapamycin", "metformin anti-aging", "GLP-1",
        "healthspan", "biological age", "caloric restriction",
        "nicotinamide riboside", "senolytics dasatinib quercetin",
        # Cancer
        "immunotherapy cancer", "CAR-T", "liquid biopsy",
        "checkpoint inhibitor", "targeted therapy cancer",
    ]

    RELEVANT_JOURNALS = [
        "Nature Aging", "Cell Metabolism", "Aging Cell",
        "GeroScience", "Lancet Healthy Longevity",
        "Nature Medicine", "Cell", "Nature", "Science",
        "Journal of Clinical Investigation", "JAMA",
        "bioRxiv", "medRxiv",  # Preprint servers
    ]

    def __init__(self):
        self.pubmed_base = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
        self.biorxiv_base = "https://api.biorxiv.org"
        self.clinicaltrials_base = "https://clinicaltrials.gov/api/v2"

    # ============ PubMed Integration ============
    async def search_pubmed(
        self,
        query: str,
        max_results: int = 20,
        days_back: int = 7
    ) -> List[Paper]:
        """Search PubMed for recent papers"""
        import aiohttp

        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        date_filter = f"{start_date.strftime('%Y/%m/%d')}:{end_date.strftime('%Y/%m/%d')}[dp]"

        search_url = f"{self.pubmed_base}/esearch.fcgi"
        params = {
            "db": "pubmed",
            "term": f"({query}) AND {date_filter}",
            "retmax": max_results,
            "retmode": "json",
            "sort": "relevance"
        }

        async with aiohttp.ClientSession() as session:
            async with session.get(search_url, params=params) as resp:
                if resp.status != 200:
                    return []
                data = await resp.json()
                ids = data.get("esearchresult", {}).get("idlist", [])

        if not ids:
            return []

        return await self._fetch_pubmed_details(ids)

    async def _fetch_pubmed_details(self, pmids: List[str]) -> List[Paper]:
        """Fetch paper details from PubMed"""
        import aiohttp
        import xml.etree.ElementTree as ET

        fetch_url = f"{self.pubmed_base}/efetch.fcgi"
        params = {
            "db": "pubmed",
            "id": ",".join(pmids),
            "retmode": "xml"
        }

        papers = []
        async with aiohttp.ClientSession() as session:
            async with session.get(fetch_url, params=params) as resp:
                if resp.status != 200:
                    return []
                xml_text = await resp.text()

        try:
            root = ET.fromstring(xml_text)
            for article in root.findall(".//PubmedArticle"):
                paper = self._parse_pubmed_article(article)
                if paper:
                    paper.topics = ["pubmed"]
                    papers.append(paper)
        except ET.ParseError:
            pass

        return papers

    def _parse_pubmed_article(self, article) -> Optional[Paper]:
        """Parse PubMed XML article"""
        try:
            medline = article.find(".//MedlineCitation")
            article_data = medline.find(".//Article")

            title = article_data.findtext(".//ArticleTitle", "")
            abstract = article_data.findtext(".//Abstract/AbstractText", "")

            authors = []
            for author in article_data.findall(".//Author"):
                last = author.findtext("LastName", "")
                first = author.findtext("ForeName", "")
                if last:
                    authors.append(f"{last} {first}".strip())

            journal = article_data.findtext(".//Journal/Title", "")
            pmid = medline.findtext(".//PMID", "")
            doi_elem = article_data.find(".//ELocationID[@EIdType='doi']")
            doi = doi_elem.text if doi_elem is not None else ""
            pub_date = medline.findtext(".//DateCompleted/Year", "")

            return Paper(
                title=title,
                authors=authors[:5],
                abstract=abstract,
                journal=journal,
                doi=doi,
                pub_date=pub_date,
                url=f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else "",
                topics=[]
            )
        except Exception:
            return None

    # ============ bioRxiv/medRxiv Integration ============
    async def search_biorxiv(
        self,
        query: str,
        max_results: int = 20,
        days_back: int = 30,
        server: str = "biorxiv"  # or "medrxiv"
    ) -> List[Paper]:
        """
        Search bioRxiv/medRxiv for preprints

        Note: bioRxiv API returns papers by date range, then we filter by query
        """
        import aiohttp

        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)

        # bioRxiv API endpoint for recent papers
        # Format: /details/[server]/[start_date]/[end_date]/[cursor]
        url = f"{self.biorxiv_base}/details/{server}/{start_date.strftime('%Y-%m-%d')}/{end_date.strftime('%Y-%m-%d')}/0/json"

        papers = []
        try:
            async with aiohttp.ClientSession() as session:
                # Paginate up to 3 pages (300 papers) for better coverage
                for cursor in range(0, 300, 100):
                    page_url = f"{self.biorxiv_base}/details/{server}/{start_date.strftime('%Y-%m-%d')}/{end_date.strftime('%Y-%m-%d')}/{cursor}/json"
                    async with session.get(page_url, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                        if resp.status != 200:
                            break
                        data = await resp.json()

                    collection = data.get("collection", [])
                    if not collection:
                        break

                    # Filter by query keywords (flexible word matching)
                    query_words = [w.lower().strip("+") for w in query.split() if len(w) >= 3]
                    for item in collection:
                        title = item.get("title", "").lower()
                        abstract = item.get("abstract", "").lower()
                        text = title + " " + abstract

                        # Match: ALL query words must appear (AND logic)
                        matches = sum(1 for w in query_words if w in text)
                        if query_words and matches >= len(query_words):
                            paper = Paper(
                                title=item.get("title", ""),
                                authors=item.get("authors", "").split("; ")[:5],
                                abstract=item.get("abstract", ""),
                                journal=f"{server} (preprint)",
                                doi=item.get("doi", ""),
                                pub_date=item.get("date", ""),
                                url=f"https://www.{server}.org/content/{item.get('doi', '')}",
                                topics=[server, "preprint"]
                            )
                            papers.append(paper)

                    if len(papers) >= max_results:
                        break

        except Exception as e:
            print(f"{server} search error: {e}")

        return papers[:max_results]

    async def search_medrxiv(
        self,
        query: str,
        max_results: int = 20,
        days_back: int = 30
    ) -> List[Paper]:
        """Search medRxiv for medical preprints"""
        return await self.search_biorxiv(query, max_results, days_back, server="medrxiv")

    # ============ ClinicalTrials.gov Integration ============
    async def search_clinical_trials(
        self,
        query: str,
        max_results: int = 20,
        status: str = "RECRUITING"  # RECRUITING, COMPLETED, ACTIVE_NOT_RECRUITING
    ) -> List[ClinicalTrial]:
        """
        Search ClinicalTrials.gov for relevant trials

        Useful for finding cutting-edge longevity interventions in human trials
        """
        import aiohttp

        url = f"{self.clinicaltrials_base}/studies"
        params = {
            "query.term": query,
            "filter.overallStatus": status,
            "pageSize": max_results,
            "format": "json"
        }

        trials = []
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                    if resp.status != 200:
                        return []
                    data = await resp.json()

            for study in data.get("studies", []):
                protocol = study.get("protocolSection", {})
                id_module = protocol.get("identificationModule", {})
                status_module = protocol.get("statusModule", {})
                desc_module = protocol.get("descriptionModule", {})
                conditions_module = protocol.get("conditionsModule", {})
                interventions_module = protocol.get("armsInterventionsModule", {})

                nct_id = id_module.get("nctId", "")

                trial = ClinicalTrial(
                    nct_id=nct_id,
                    title=id_module.get("briefTitle", ""),
                    status=status_module.get("overallStatus", ""),
                    phase=", ".join(protocol.get("designModule", {}).get("phases", [])),
                    conditions=conditions_module.get("conditions", []),
                    interventions=[
                        i.get("name", "") for i in
                        interventions_module.get("interventions", [])
                    ],
                    summary=desc_module.get("briefSummary", ""),
                    start_date=status_module.get("startDateStruct", {}).get("date", ""),
                    url=f"https://clinicaltrials.gov/study/{nct_id}"
                )
                trials.append(trial)

        except Exception as e:
            print(f"ClinicalTrials.gov search error: {e}")

        return trials

    def clinical_trial_to_paper(self, trial: ClinicalTrial) -> Paper:
        """Convert ClinicalTrial to Paper format for unified processing"""
        return Paper(
            title=f"[Clinical Trial] {trial.title}",
            authors=[],
            abstract=trial.summary,
            journal=f"ClinicalTrials.gov ({trial.phase})",
            doi=trial.nct_id,
            pub_date=trial.start_date,
            url=trial.url,
            topics=["clinical_trial"] + trial.conditions[:3]
        )

    # ============ Unified Multi-Source Search ============
    async def search_all_sources(
        self,
        query: str,
        max_per_source: int = 10,
        days_back: int = 14,
        include_trials: bool = True
    ) -> Dict[str, List]:
        """
        Search all sources in parallel

        Returns:
            {
                "pubmed": [Paper, ...],
                "biorxiv": [Paper, ...],
                "medrxiv": [Paper, ...],
                "clinical_trials": [ClinicalTrial, ...]
            }
        """
        tasks = [
            self.search_pubmed(query, max_per_source, days_back),
            self.search_biorxiv(query, max_per_source, days_back * 2),  # wider window for preprints
            self.search_medrxiv(query, max_per_source, days_back * 2),
        ]

        if include_trials:
            # Use simpler query for clinical trials (first significant word)
            trial_query = query.split()[0] if query else "longevity"
            tasks.append(self.search_clinical_trials(trial_query, max_per_source))

        results = await asyncio.gather(*tasks, return_exceptions=True)

        return {
            "pubmed": results[0] if not isinstance(results[0], Exception) else [],
            "biorxiv": results[1] if not isinstance(results[1], Exception) else [],
            "medrxiv": results[2] if not isinstance(results[2], Exception) else [],
            "clinical_trials": results[3] if len(results) > 3 and not isinstance(results[3], Exception) else []
        }

    async def get_weekly_papers(self, include_preprints: bool = True, include_trials: bool = True) -> List[Paper]:
        """
        Get this week's relevant papers from all sources

        Args:
            include_preprints: Include bioRxiv/medRxiv preprints
            include_trials: Include clinical trials (converted to Paper format)
        """
        all_papers = []
        print("ğŸ“š Searching multiple sources...")

        # Search each source for top keywords
        for keyword in self.LONGEVITY_KEYWORDS[:5]:
            print(f"   ğŸ” Keyword: {keyword}")

            # PubMed (always)
            pubmed_papers = await self.search_pubmed(keyword, max_results=10, days_back=7)
            for p in pubmed_papers:
                if not p.topics or "pubmed" not in p.topics:
                    p.topics = ["pubmed"]
            all_papers.extend(pubmed_papers)

            if include_preprints:
                # bioRxiv
                biorxiv_papers = await self.search_biorxiv(keyword, max_results=5, days_back=30)
                all_papers.extend(biorxiv_papers)

                # medRxiv (30 days, broader window for medical preprints)
                medrxiv_papers = await self.search_medrxiv(keyword, max_results=5, days_back=30)
                all_papers.extend(medrxiv_papers)

        # Clinical trials use broader keywords for better results
        if include_trials:
            for keyword in self.CLINICAL_TRIAL_KEYWORDS[:6]:
                print(f"   ğŸ¥ Clinical trial keyword: {keyword}")
                trials = await self.search_clinical_trials(keyword, max_results=5)
                for trial in trials:
                    all_papers.append(self.clinical_trial_to_paper(trial))

        # Deduplicate by DOI
        seen_dois = set()
        unique_papers = []
        for paper in all_papers:
            key = paper.doi or paper.title
            if key and key not in seen_dois:
                seen_dois.add(key)
                unique_papers.append(paper)

        # Score and rank
        for paper in unique_papers:
            paper.relevance_score = self._calculate_relevance(paper)

        print(f"   âœ… Found {len(unique_papers)} unique papers")

        # Ensure source diversity: reserve slots for each source
        sorted_papers = sorted(unique_papers, key=lambda p: p.relevance_score, reverse=True)

        # Group by source
        by_source = {"pubmed": [], "biorxiv": [], "medrxiv": [], "clinical_trial": []}
        for paper in sorted_papers:
            src = "pubmed"
            for t in (paper.topics or []):
                if t in by_source:
                    src = t
                    break
            by_source[src].append(paper)

        # Guarantee minimum representation: 2 per non-empty source, fill rest by score
        selected = []
        used = set()
        for source in ["clinical_trial", "medrxiv", "biorxiv", "pubmed"]:
            for paper in by_source[source][:3]:  # up to 3 per source guaranteed
                key = paper.doi or paper.title
                if key not in used:
                    selected.append(paper)
                    used.add(key)

        # Fill remaining slots with highest-scored papers
        for paper in sorted_papers:
            if len(selected) >= 15:
                break
            key = paper.doi or paper.title
            if key not in used:
                selected.append(paper)
                used.add(key)

        return selected[:15]

    def _calculate_relevance(self, paper: Paper) -> float:
        """Calculate relevance score for a paper"""
        score = 0.0

        # Source bonus
        if paper.topics:
            if "clinical_trial" in paper.topics:
                score += 2.0  # Clinical trials are high value
            if "preprint" in paper.topics:
                score += 0.5  # Preprints are cutting edge

        # Journal impact
        if paper.journal:
            for journal in self.RELEVANT_JOURNALS:
                if journal.lower() in paper.journal.lower():
                    score += 3.0
                    break

        # Keyword matches in title
        title_lower = paper.title.lower()
        for keyword in self.LONGEVITY_KEYWORDS:
            if keyword.lower() in title_lower:
                score += 1.0

        # Abstract quality
        if paper.abstract and len(paper.abstract) > 500:
            score += 1.0

        return score


class DocumentProcessor:
    """
    ë¬¸ì„œ ì²˜ë¦¬ê¸° (Docling ê¸°ë°˜)
    Process PDFs and documents using Docling
    """

    def __init__(self):
        self.docling_available = self._check_docling()

    def _check_docling(self) -> bool:
        """Check if docling is available"""
        try:
            from docling.document_converter import DocumentConverter
            return True
        except ImportError:
            return False

    async def process_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """Process a PDF file and extract structured content"""
        if not self.docling_available:
            return {"error": "Docling not installed. Run: pip install docling"}

        from docling.document_converter import DocumentConverter

        converter = DocumentConverter()
        result = converter.convert(pdf_path)
        doc = result.document

        return {
            "markdown": doc.export_to_markdown(),
            "text": doc.export_to_text(),
            "tables": [table.export_to_dataframe().to_dict() for table in doc.tables],
            "figures": [{"caption": fig.caption} for fig in doc.pictures],
            "metadata": {
                "title": doc.title,
                "pages": len(doc.pages) if hasattr(doc, 'pages') else 0
            }
        }


class ContentGenerator:
    """
    AI ì½˜í…ì¸  ìƒì„±ê¸°
    Generates content using Gemini or GPT-4

    í•œêµ­ì–´ë¡œ ë”°ëœ»í•˜ê³  ì „ë¬¸ì ì¸ ì½˜í…ì¸  ìƒì„±
    """

    SYSTEM_PROMPT_KOREAN = """ë‹¹ì‹ ì€ ë¸Œë¼ìš´ë°”ì´ì˜¤í…(Brown Biotech)ì˜ ì¥ìˆ˜ê³¼í•™ ë¦¬ì„œì¹˜íŒ€ ê´€ì ìœ¼ë¡œ ê¸€ì„ ì‘ì„±í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì—­í• :
- ìµœì‹  ì˜í•™ ì—°êµ¬ë¥¼ ëŒ€ì¤‘ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
- ê³¼í•™ì  ì •í™•ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ë”°ëœ»í•˜ê³  ê³µê°ì ì¸ í†¤ ìœ ì§€
- ì‹¤ìš©ì ì¸ ê±´ê°• ì¸ì‚¬ì´íŠ¸ ì œê³µ

ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼:
- ì „ë¬¸ ìš©ì–´ëŠ” ì‰¬ìš´ ì„¤ëª…ê³¼ í•¨ê»˜ ì‚¬ìš©
- ë…ìì™€ ëŒ€í™”í•˜ë“¯ ì¹œê·¼í•˜ê²Œ (ì¡´ëŒ“ë§ ì‚¬ìš©)
- í•µì‹¬ ë©”ì‹œì§€ë¥¼ ëª…í™•í•˜ê²Œ
- í¬ë§ì ì´ê³  ê¸ì •ì ì¸ í†¤ ìœ ì§€

ì£¼ì˜ì‚¬í•­:
- ì˜ë£Œì  ì¡°ì–¸ì€ "~í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤", "~ë¼ê³  í•©ë‹ˆë‹¤" í˜•ì‹ìœ¼ë¡œ
- ì¶œì²˜ë¥¼ ëª…í™•íˆ ë°íˆê¸°
- ê³¼ì¥í•˜ê±°ë‚˜ í™•ì •ì ì¸ í‘œí˜„ ìì œ"""

    CONTENT_TEMPLATES = {
        "newsletter": """
## ë‰´ìŠ¤ë ˆí„° í˜•ì‹

ì œëª©: [í¥ë¯¸ë¥¼ ë„ëŠ” ì§ˆë¬¸ í˜•ì‹ì˜ ì œëª©]

ì¸ì‚¬ë§:
ì•ˆë…•í•˜ì„¸ìš”, ë¸Œë¼ìš´ë°”ì´ì˜¤í…ì…ë‹ˆë‹¤.
ì´ë²ˆ ì£¼ì— ì£¼ëª©í•  ë§Œí•œ ì—°êµ¬ê°€ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤.

í•µì‹¬ ë‚´ìš© (3-4 ë¬¸ë‹¨):
- ì—°êµ¬ ë°°ê²½ê³¼ ì¤‘ìš”ì„±
- ì£¼ìš” ë°œê²¬ ë‚´ìš©
- ìš°ë¦¬ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ì˜ë¯¸
- ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” ì 

ë§ˆë¬´ë¦¬:
ê±´ê°•í•œ í•œ ì£¼ ë³´ë‚´ì‹œê¸° ë°”ëë‹ˆë‹¤.

ì´ ê¸¸ì´: 400-600ì
""",
        "blog": """
## ë¸”ë¡œê·¸ í˜•ì‹

ì œëª©: [SEO ìµœì í™”ëœ ì œëª©]
ë¶€ì œ: [ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” í•œ ì¤„]

ë„ì…ë¶€:
- ë…ìì˜ ê´€ì‹¬ì„ ë„ëŠ” ì§ˆë¬¸ì´ë‚˜ ìƒí™© ì œì‹œ
- ì´ ì—°êµ¬ê°€ ì™œ ì¤‘ìš”í•œì§€

ë³¸ë¬¸ (5-7 ë¬¸ë‹¨):
1. ì—°êµ¬ ë°°ê²½
2. ë°©ë²•ë¡  (ê°„ëµíˆ)
3. ì£¼ìš” ê²°ê³¼
4. ì „ë¬¸ê°€ í•´ì„
5. í•œê³„ì  (ê· í˜• ì¡íŒ ì‹œê°)
6. ì‹¤ìƒí™œ ì ìš©

ê²°ë¡ :
- í•µì‹¬ ë©”ì‹œì§€ ìš”ì•½
- ë…ì í–‰ë™ ìœ ë„ (CTA)

ì´ ê¸¸ì´: 800-1200ì
""",
        "youtube_script": """
## ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ í˜•ì‹

[ì˜¤í”„ë‹ - 10ì´ˆ]
"ì—¬ëŸ¬ë¶„, ì˜¤ëŠ˜ ë°œí‘œëœ ì¶©ê²©ì ì¸ ì—°êµ¬ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤..."

[í•µì‹¬ ì§ˆë¬¸ - 20ì´ˆ]
"[ì—°êµ¬ ì£¼ì œ]ê°€ ì •ë§ [íš¨ê³¼]ì— ë„ì›€ì´ ë ê¹Œìš”?"

[ì—°êµ¬ ì†Œê°œ - 1ë¶„]
- ì–´ë””ì„œ, ëˆ„ê°€, ì–´ë–»ê²Œ ì—°êµ¬í–ˆëŠ”ì§€
- ì™œ ì´ ì—°êµ¬ê°€ ì¤‘ìš”í•œì§€

[ê²°ê³¼ ì„¤ëª… - 2ë¶„]
- í•µì‹¬ ë°œê²¬ 1, 2, 3
- ê·¸ë˜í”„/í‘œ ì„¤ëª… ì‹œì  í‘œì‹œ [ìë§‰: ...]

[ì˜ë¯¸ í•´ì„ - 1ë¶„]
- ìš°ë¦¬ì—ê²Œ ì–´ë–¤ ì˜ë¯¸ì¸ì§€
- ì£¼ì˜í•  ì 

[ì‹¤ì²œ íŒ - 1ë¶„]
- ì˜¤ëŠ˜ë¶€í„° í•  ìˆ˜ ìˆëŠ” ê²ƒ

[í´ë¡œì§• - 30ì´ˆ]
- êµ¬ë…/ì¢‹ì•„ìš” ìš”ì²­
- ë‹¤ìŒ ì˜ìƒ ì˜ˆê³ 

ì´ ê¸¸ì´: 5-7ë¶„ ë¶„ëŸ‰
"""
    }

    def __init__(self, provider: str = "gemini"):
        """
        Initialize content generator

        Args:
            provider: "gemini", "openai", or "kimi"
        """
        self.provider = provider

    def _provider_has_key(self, provider: str) -> bool:
        if provider == "kimi":
            return bool(KIMI_API_KEY)
        if provider == "gemini":
            return bool(GEMINI_API_KEY)
        if provider == "openai":
            return bool(OPENAI_API_KEY)
        return False

    def _fallback_provider(self) -> Optional[str]:
        """
        Choose a reasonable fallback provider when the primary provider errors.
        Preference: kimi -> gemini -> openai (cost first), but only if keys exist.
        """
        for p in ("kimi", "gemini", "openai"):
            if p != self.provider and self._provider_has_key(p):
                return p
        return None

    async def generate_content(
        self,
        paper: Paper,
        content_type: str = "newsletter",
        language: str = "korean"
    ) -> ContentDraft:
        """Generate content from a research paper"""

        template = self.CONTENT_TEMPLATES.get(content_type, self.CONTENT_TEMPLATES["newsletter"])

        prompt = f"""ë‹¤ìŒ ì—°êµ¬ ë…¼ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ {content_type} ì½˜í…ì¸ ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

## ë…¼ë¬¸ ì •ë³´
ì œëª©: {paper.title}
ì €ì: {', '.join(paper.authors)}
ì €ë„: {paper.journal}
ë°œí–‰ì¼: {paper.pub_date}

## ì´ˆë¡
{paper.abstract}

## ì‘ì„± í˜•ì‹
{template}

## ì¶”ê°€ ìš”ì²­ì‚¬í•­
1. í•œêµ­ì–´ë¡œ ì‘ì„±
2. ì „ë¬¸ ìš©ì–´ëŠ” ì˜ì–´ ì›ë¬¸ì„ ê´„í˜¸ë¡œ ë³‘ê¸°
3. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 3ê°€ì§€ ë³„ë„ ì •ë¦¬
4. ì‹¤ìš©ì  ì ìš© ë°©ë²• 2-3ê°€ì§€ ì œì•ˆ
5. ì¸ìš© ì¶œì²˜ ëª…ì‹œ

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "korean_title": "ì œëª©",
    "korean_summary": "2-3ë¬¸ì¥ ìš”ì•½",
    "korean_body": "ë³¸ë¬¸ ì „ì²´",
    "key_insights": ["ì¸ì‚¬ì´íŠ¸1", "ì¸ì‚¬ì´íŠ¸2", "ì¸ì‚¬ì´íŠ¸3"],
    "practical_applications": ["ì ìš©1", "ì ìš©2"],
    "confidence_score": 0.0-1.0
}}"""

        async def call_with_provider(provider: str) -> str:
            if provider == "gemini":
                return await self._call_gemini(prompt)
            if provider == "kimi":
                return await self._call_kimi(prompt)
            return await self._call_openai(prompt)

        response: str
        try:
            response = await call_with_provider(self.provider)
        except Exception:
            fb = self._fallback_provider()
            if not fb:
                raise
            response = await call_with_provider(fb)

        try:
            content_data = parse_json_response(response)
        except json.JSONDecodeError:
            # Treat malformed output as a hard failure so we can try another paper/provider.
            raise ValueError("AI response was not valid JSON")

        # Determine source from paper topics
        source = "pubmed"  # default
        if paper.topics:
            if "clinical_trial" in paper.topics:
                source = "clinical_trial"
            elif "medrxiv" in paper.topics:
                source = "medrxiv"
            elif "biorxiv" in paper.topics:
                source = "biorxiv"

        return ContentDraft(
            paper=paper,
            content_type=content_type,
            korean_title=content_data.get("korean_title", ""),
            korean_summary=content_data.get("korean_summary", ""),
            korean_body=content_data.get("korean_body", ""),
            english_title=paper.title,
            english_summary=paper.abstract,  # Full abstract, not truncated
            key_insights=content_data.get("key_insights", []),
            practical_applications=content_data.get("practical_applications", []),
            citations=[{"doi": paper.doi, "title": paper.title, "journal": paper.journal}],
            fact_check_notes=[],
            confidence_score=content_data.get("confidence_score", 0.5),
            created_at=datetime.now().isoformat(),
            status="draft",
            source=source,
        )

    async def _call_gemini(self, prompt: str, max_retries: int = 3) -> str:
        """Call Gemini API with retry on rate limit (new google.genai SDK)"""
        from google import genai
        from google.genai import types

        client = genai.Client(api_key=GEMINI_API_KEY)

        for attempt in range(max_retries):
            try:
                response = client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=[self.SYSTEM_PROMPT_KOREAN + "\n\n" + prompt],
                    config=types.GenerateContentConfig(
                        temperature=0.7,
                        max_output_tokens=8192,
                        safety_settings=[
                            types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                            types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                            types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                            types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                        ],
                    ),
                )
                if not getattr(response, "text", None):
                    raise RuntimeError("Gemini returned empty response")
                return response.text
            except Exception as e:
                error_str = str(e)
                if "429" in error_str and attempt < max_retries - 1:
                    wait = (attempt + 1) * 15
                    print(f"   â³ Rate limit, {wait}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})...")
                    await asyncio.sleep(wait)
                    continue
                raise RuntimeError(f"Gemini API error: {e}") from e

    async def _call_openai(self, prompt: str) -> str:
        """Call OpenAI API"""
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=OPENAI_API_KEY)

        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": self.SYSTEM_PROMPT_KOREAN},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=2048
        )
        content = response.choices[0].message.content
        if not content:
            raise RuntimeError("OpenAI returned empty response")
        return content

    async def _call_kimi(self, prompt: str) -> str:
        """
        Call Kimi (Moonshot AI) API - ì €ë ´í•œ ëŒ€ì•ˆ

        Pricing (2026):
        - Input: $0.45-0.60 / 1M tokens
        - Output: $2.50 / 1M tokens
        - 75% discount with caching

        vs OpenAI GPT-4o:
        - Input: $2.50 / 1M tokens
        - Output: $10.00 / 1M tokens
        """
        from openai import AsyncOpenAI

        # Kimi uses OpenAI-compatible API
        client = AsyncOpenAI(
            api_key=KIMI_API_KEY,
            base_url="https://api.moonshot.ai/v1",
            timeout=120.0  # Kimi can be slower, allow 2 min
        )

        response = await client.chat.completions.create(
            model="moonshot-v1-8k",  # Use 8k for faster response
            messages=[
                {"role": "system", "content": self.SYSTEM_PROMPT_KOREAN},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=2048
        )
        content = response.choices[0].message.content
        if not content:
            raise RuntimeError("Kimi returned empty response")
        return content

    async def revise_content(
        self,
        paper: Paper,
        draft: ContentDraft,
        issues: List[str],
        content_type: str = "newsletter",
    ) -> ContentDraft:
        """
        Try to auto-fix a draft based on fact-check issues.
        This is intentionally conservative: prefer removing/qualifying claims over inventing details.
        """
        issues_text = "\n".join(f"- {i}" for i in (issues or [])[:8])
        prompt = f"""ë‹¤ìŒ ì½˜í…ì¸ ë¥¼ ì›ë³¸ ì´ˆë¡ ë²”ìœ„ ë‚´ì—ì„œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.

ì¤‘ìš”:
- ì´ˆë¡/ì œëª©ì— ì—†ëŠ” ìˆ«ì, ì €ì, ì—°ë„, ê²°ê³¼ë¥¼ ìƒˆë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
- í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ 'ì´ˆë¡ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤'ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.
- ìš©ì–´ëŠ” ì›ë¬¸ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ì„¸ìš”(ì˜ˆ: knockout vs inhibition ë“±).
- ì¶œë ¥ì€ ë°˜ë“œì‹œ JSONë§Œ(ì½”ë“œë¸”ë¡/ì„¤ëª… ì—†ì´) ë°˜í™˜í•˜ì„¸ìš”.

## ì›ë³¸ ë…¼ë¬¸
ì œëª©: {paper.title}
ì €ì: {', '.join(paper.authors)}
ì €ë„: {paper.journal}
ë°œí–‰ì¼: {paper.pub_date}

ì´ˆë¡:
{paper.abstract}

## ê¸°ì¡´ ì½˜í…ì¸ (JSON ì¼ë¶€)
korean_title: {draft.korean_title}
korean_summary: {draft.korean_summary}
korean_body:
{draft.korean_body}

## íŒ©íŠ¸ì²´í¬ ì´ìŠˆ
{issues_text}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "korean_title": "...",
  "korean_summary": "...",
  "korean_body": "...",
  "key_insights": ["...", "...", "..."],
  "practical_applications": ["...", "..."],
  "confidence_score": 0.0-1.0
}}"""

        original_provider = self.provider
        try:
            response = await (self._call_gemini(prompt) if self.provider == "gemini"
                              else self._call_kimi(prompt) if self.provider == "kimi"
                              else self._call_openai(prompt))
        except Exception:
            fb = self._fallback_provider()
            if not fb:
                raise
            self.provider = fb
            response = await (self._call_gemini(prompt) if self.provider == "gemini"
                              else self._call_kimi(prompt) if self.provider == "kimi"
                              else self._call_openai(prompt))
        finally:
            self.provider = original_provider

        content_data = parse_json_response(response)
        draft.korean_title = content_data.get("korean_title", draft.korean_title)
        draft.korean_summary = content_data.get("korean_summary", draft.korean_summary)
        draft.korean_body = content_data.get("korean_body", draft.korean_body)
        draft.key_insights = content_data.get("key_insights", draft.key_insights)
        draft.practical_applications = content_data.get("practical_applications", draft.practical_applications)
        draft.confidence_score = content_data.get("confidence_score", draft.confidence_score)
        return draft


class FactChecker:
    """
    íŒ©íŠ¸ ì²´ì»¤
    Validates content accuracy using GPT-4 or Kimi (cheaper)
    """

    def __init__(self, provider: str = "gemini"):
        """
        Initialize fact checker

        Args:
            provider: "gemini", "openai", or "kimi"
        """
        self.provider = provider

    FACT_CHECK_PROMPT = """ë‹¹ì‹ ì€ ì˜í•™ ë…¼ë¬¸ íŒ©íŠ¸ì²´ì»¤ì…ë‹ˆë‹¤.

ë‹¤ìŒ AI ìƒì„± ì½˜í…ì¸ ê°€ ì›ë³¸ ë…¼ë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦í•´ì£¼ì„¸ìš”.

## ì›ë³¸ ë…¼ë¬¸
ì œëª©: {title}
ì´ˆë¡: {abstract}

## AI ìƒì„± ì½˜í…ì¸ 
{content}

## ê²€ì¦ í•­ëª©
1. ìˆ«ì/í†µê³„ ì •í™•ì„±
2. ì¸ê³¼ê´€ê³„ ì™œê³¡ ì—¬ë¶€
3. ê³¼ì¥ëœ í‘œí˜„ ì—¬ë¶€
4. ëˆ„ë½ëœ ì¤‘ìš” ì •ë³´
5. ì ì¬ì  ì˜¤í•´ ê°€ëŠ¥ì„±

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "accuracy_score": 0.0-1.0,
    "issues": ["ë¬¸ì œì 1", "ë¬¸ì œì 2"],
    "suggestions": ["ìˆ˜ì •ì œì•ˆ1", "ìˆ˜ì •ì œì•ˆ2"],
    "safe_to_publish": true/false
}}"""

    async def check(self, draft: ContentDraft) -> Dict[str, Any]:
        """Fact-check a content draft"""
        prompt = self.FACT_CHECK_PROMPT.format(
            title=draft.paper.title,
            abstract=draft.paper.abstract,
            content=draft.korean_body
        )

        if self.provider == "kimi":
            return await self._check_with_kimi(prompt)
        elif self.provider == "gemini":
            return await self._check_with_gemini(prompt)
        else:
            return await self._check_with_openai(prompt)

    async def _check_with_gemini(self, prompt: str) -> Dict[str, Any]:
        """Fact-check using Gemini (new google.genai SDK)"""
        try:
            from google import genai
            from google.genai import types

            client = genai.Client(api_key=GEMINI_API_KEY)

            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=[prompt],
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    max_output_tokens=4096,
                    safety_settings=[
                        types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                    ],
                ),
            )
            return parse_json_response(response.text)
        except json.JSONDecodeError:
            return {
                "accuracy_score": 0.7,
                "issues": ["JSON íŒŒì‹± ì‹¤íŒ¨ - ìˆ˜ë™ ê²€í†  í•„ìš”"],
                "suggestions": [],
                "safe_to_publish": False
            }
        except Exception as e:
            return {
                "accuracy_score": 0.0,
                "issues": [f"Fact check failed (Gemini): {e}"],
                "suggestions": [],
                "safe_to_publish": False
            }

    async def _check_with_openai(self, prompt: str) -> Dict[str, Any]:
        """Fact-check using OpenAI GPT-4"""
        try:
            from openai import AsyncOpenAI

            client = AsyncOpenAI(api_key=OPENAI_API_KEY)

            response = await client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1024
            )

            return parse_json_response(response.choices[0].message.content)
        except Exception as e:
            return {
                "accuracy_score": 0.0,
                "issues": [f"Fact check failed: {e}"],
                "suggestions": [],
                "safe_to_publish": False
            }

    async def _check_with_kimi(self, prompt: str) -> Dict[str, Any]:
        """Fact-check using Kimi (cheaper alternative)"""
        try:
            from openai import AsyncOpenAI

            client = AsyncOpenAI(
                api_key=KIMI_API_KEY,
                base_url="https://api.moonshot.ai/v1",
                timeout=120.0
            )

            response = await client.chat.completions.create(
                model="moonshot-v1-8k",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1024
            )

            return parse_json_response(response.choices[0].message.content)
        except Exception as e:
            return {
                "accuracy_score": 0.0,
                "issues": [f"Fact check failed (Kimi): {e}"],
                "suggestions": [],
                "safe_to_publish": False
            }


class ContentPipeline:
    """
    ì „ì²´ ì½˜í…ì¸  íŒŒì´í”„ë¼ì¸
    Orchestrates the entire content generation workflow

    Provider options:
    - "gemini": Google Gemini (default, moderate cost)
    - "openai": OpenAI GPT-4 (highest quality, highest cost)
    - "kimi": Moonshot Kimi (good quality, lowest cost - recommended for budget)
    """

    def __init__(self, ai_provider: str = "gemini", fact_check_provider: str = None):
        """
        Initialize content pipeline

        Args:
            ai_provider: Provider for content generation ("gemini", "openai", "kimi")
            fact_check_provider: Provider for fact checking (defaults to ai_provider)
        """
        self.discovery = PaperDiscovery()
        self.doc_processor = DocumentProcessor()
        self.generator = ContentGenerator(provider=ai_provider)
        # Use same provider for fact-check by default
        fc_provider = fact_check_provider or ai_provider
        self.fact_checker = FactChecker(provider=fc_provider)

    async def run_weekly_pipeline(
        self,
        include_preprints: bool = True,
        include_trials: bool = True
    ) -> List[ContentDraft]:
        """
        ì£¼ê°„ ì½˜í…ì¸  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë©€í‹°ì†ŒìŠ¤)

        ë°ì´í„° ì†ŒìŠ¤:
        - PubMed: í”¼ì–´ë¦¬ë·° ë…¼ë¬¸
        - bioRxiv/medRxiv: í”„ë¦¬í”„ë¦°íŠ¸ (ìµœì‹  ì—°êµ¬)
        - ClinicalTrials.gov: ì§„í–‰ ì¤‘ì¸ ì„ìƒì‹œí—˜

        íŒŒì´í”„ë¼ì¸:
        1. ë©€í‹°ì†ŒìŠ¤ ë…¼ë¬¸ ìˆ˜ì§‘
        2. ìƒìœ„ 5ê°œ ì„ ì •
        3. ê° ë…¼ë¬¸ë³„ ë‰´ìŠ¤ë ˆí„° ì½˜í…ì¸  ìƒì„±
        4. íŒ©íŠ¸ì²´í¬
        5. ê²€í†  ëŒ€ê¸° ìƒíƒœë¡œ ì €ì¥
        """
        print("ğŸ” ë©€í‹°ì†ŒìŠ¤ ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘...")
        print("   ğŸ“š Sources: PubMed", end="")
        if include_preprints:
            print(" + bioRxiv + medRxiv", end="")
        if include_trials:
            print(" + ClinicalTrials.gov", end="")
        print()

        papers = await self.discovery.get_weekly_papers(
            include_preprints=include_preprints,
            include_trials=include_trials
        )

        # Show source breakdown
        source_counts = {"pubmed": 0, "biorxiv": 0, "medrxiv": 0, "clinical_trial": 0}
        for paper in papers:
            for topic in (paper.topics or []):
                if topic in source_counts:
                    source_counts[topic] += 1
                    break  # count each paper once

        print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        for source, count in source_counts.items():
            icon = {"pubmed": "ğŸ“„", "biorxiv": "ğŸ§¬", "medrxiv": "ğŸ¥", "clinical_trial": "ğŸ’Š"}.get(source, "â€¢")
            print(f"   {icon} {source}: {count}ê°œ")
        print(f"   ì´ {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")

        # Target number of publishable drafts. If a draft can't be auto-fixed, try the next paper.
        target_ready = int(os.getenv("TARGET_READY_DRAFTS", "5"))
        max_papers = int(os.getenv("MAX_PAPERS_TO_PROCESS", "15"))
        max_revisions = int(os.getenv("MAX_AUTO_REVISIONS", "2"))

        drafts: List[ContentDraft] = []
        ready_count = 0
        processed = 0

        for paper in papers:
            if processed >= max_papers or ready_count >= target_ready:
                break
            processed += 1

            print(f"\nğŸ“ ì½˜í…ì¸  ìƒì„± ì¤‘ ({ready_count + 1}/{target_ready}): {paper.title[:50]}...")

            try:
                draft = await self.generator.generate_content(
                    paper,
                    content_type="newsletter",
                    language="korean"
                )
            except Exception as e:
                print(f"   âš ï¸ ìƒì„± ì‹¤íŒ¨(ê±´ë„ˆëœ€): {e}")
                continue

            # Fact check + auto-revise loop
            for attempt in range(max_revisions + 1):
                print(f"   âœ“ íŒ©íŠ¸ì²´í¬ ì¤‘...")
                fact_result = await self.fact_checker.check(draft)
                draft.fact_check_notes = fact_result.get("issues", [])

                if fact_result.get("safe_to_publish", False):
                    draft.status = "ready_for_review"
                    print(f"   âœ… ê²€í†  ì¤€ë¹„ ì™„ë£Œ (ì •í™•ë„: {fact_result.get('accuracy_score', 0):.0%})")
                    break

                # Auto-revise if we still have attempts left
                if attempt < max_revisions and draft.fact_check_notes:
                    print(f"   ğŸ” ìë™ ìˆ˜ì • ì‹œë„ ({attempt + 1}/{max_revisions})...")
                    try:
                        draft = await self.generator.revise_content(
                            paper=paper,
                            draft=draft,
                            issues=draft.fact_check_notes,
                            content_type="newsletter",
                        )
                        continue
                    except Exception as e:
                        print(f"   âš ï¸ ìë™ ìˆ˜ì • ì‹¤íŒ¨: {e}")

                draft.status = "needs_revision"
                if draft.fact_check_notes:
                    print(f"   âš ï¸ ìˆ˜ì • í•„ìš”: {', '.join(draft.fact_check_notes[:2])}")
                else:
                    print(f"   âš ï¸ ìˆ˜ì • í•„ìš”: íŒ©íŠ¸ì²´í¬ ì‹¤íŒ¨")
                break

            drafts.append(draft)
            if draft.status == "ready_for_review":
                ready_count += 1

            # Rate limit ë°©ì§€: ë…¼ë¬¸ ì‚¬ì´ 10ì´ˆ ëŒ€ê¸°
            if ready_count < target_ready:
                await asyncio.sleep(10)

        return drafts

    async def generate_single_content(
        self,
        paper: Paper,
        content_type: str = "newsletter"
    ) -> ContentDraft:
        """ë‹¨ì¼ ë…¼ë¬¸ ì½˜í…ì¸  ìƒì„±"""
        draft = await self.generator.generate_content(paper, content_type)
        fact_result = await self.fact_checker.check(draft)
        draft.fact_check_notes = fact_result.get("issues", [])
        draft.status = "ready_for_review" if fact_result.get("safe_to_publish") else "needs_revision"
        return draft

    def save_drafts(self, drafts: List[ContentDraft], output_dir: str = "content_drafts"):
        """Save drafts to JSON files"""
        os.makedirs(output_dir, exist_ok=True)

        for draft in drafts:
            filename = f"{draft.created_at[:10]}_{draft.content_type}_{draft.paper.doi.replace('/', '_')[:20]}.json"
            filepath = os.path.join(output_dir, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(asdict(draft), f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ {len(drafts)}ê°œ ì½˜í…ì¸  ì €ì¥ ì™„ë£Œ: {output_dir}/")


# Example usage
async def main():
    """Example: Run weekly content pipeline with multi-source discovery"""
    print("=" * 60)
    print("ğŸ§¬ ì¥ìˆ˜ ì§€ì‹ í”Œë«í¼ - ìë™ ì½˜í…ì¸  íŒŒì´í”„ë¼ì¸")
    print("=" * 60)

    # Source configuration
    include_preprints = os.getenv("INCLUDE_PREPRINTS", "true").lower() == "true"
    include_trials = os.getenv("INCLUDE_TRIALS", "true").lower() == "true"

    print("\nğŸ“¡ ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •:")
    print(f"   â€¢ PubMed: âœ… (í•­ìƒ í™œì„±)")
    print(f"   â€¢ bioRxiv/medRxiv: {'âœ…' if include_preprints else 'âŒ'}")
    print(f"   â€¢ ClinicalTrials.gov: {'âœ…' if include_trials else 'âŒ'}")

    # Provider ì„ íƒ (ë¹„ìš© ìˆœ: kimi < gemini < openai)
    ai_provider = os.getenv("AI_PROVIDER", "kimi")
    if not os.getenv("KIMI_API_KEY") and ai_provider == "kimi":
        ai_provider = "gemini"  # Fallback to gemini
    if not os.getenv("GEMINI_API_KEY") and ai_provider == "gemini":
        ai_provider = "openai"  # Fallback to openai

    # Fact-check provider should be stable and allowed to be different from generation.
    fact_check_provider = os.getenv("FACT_CHECK_PROVIDER", "").strip().lower() or ""
    if fact_check_provider not in ("kimi", "gemini", "openai"):
        fact_check_provider = ""
    if not fact_check_provider:
        # Prefer Kimi for cost, then OpenAI, then fall back to generation provider.
        if os.getenv("KIMI_API_KEY"):
            fact_check_provider = "kimi"
        elif os.getenv("OPENAI_API_KEY"):
            fact_check_provider = "openai"
        else:
            fact_check_provider = ai_provider

    print(f"\nğŸ¤– AI Provider: {ai_provider} (fact-check: {fact_check_provider})")
    pipeline = ContentPipeline(ai_provider=ai_provider, fact_check_provider=fact_check_provider)

    # Run weekly pipeline with multi-source
    drafts = await pipeline.run_weekly_pipeline(
        include_preprints=include_preprints,
        include_trials=include_trials
    )

    # Save drafts
    pipeline.save_drafts(drafts)

    # Generate Instagram card news images
    try:
        from card_news_generator import CardNewsGenerator
        print("\nğŸ¨ Instagram ì¹´ë“œë‰´ìŠ¤ ìƒì„± ì¤‘...")
        card_gen = CardNewsGenerator()
        for draft in drafts:
            paths = card_gen.generate_for_draft(draft)
            print(f"   ğŸ“¸ {len(paths)} slides: {draft.korean_title[:40]}...")
        print("âœ… ì¹´ë“œë‰´ìŠ¤ ìƒì„± ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ ì¹´ë“œë‰´ìŠ¤ ìƒì„± ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")

    # Instagram auto-posting (opt-in via env var)
    instagram_enabled = os.getenv("INSTAGRAM_AUTO_POST", "false").lower() == "true"
    if instagram_enabled:
        try:
            from instagram_poster import InstagramPoster
            print("\nğŸ“± Instagram ìë™ í¬ìŠ¤íŒ… ì¤‘...")
            poster = InstagramPoster(
                ig_user_id=os.getenv("INSTAGRAM_USER_ID"),
                access_token=os.getenv("INSTAGRAM_ACCESS_TOKEN"),
                github_repo=os.getenv("GITHUB_REPOSITORY", "chang-myungoh/longevity-platform"),
                pages_base_url=os.getenv("PAGES_BASE_URL", "https://chang-myungoh.github.io/longevity-platform"),
            )

            # Build list of (draft_data, card_news_dir) pairs
            drafts_with_dirs = []
            card_news_base = os.path.join(os.path.dirname(__file__), "card_news")
            for draft in drafts:
                from dataclasses import asdict as _asdict
                draft_data = _asdict(draft)
                # Reconstruct card_news dir name (matches CardNewsGenerator logic)
                date_str = draft.created_at[:10] if draft.created_at else datetime.now().strftime("%Y-%m-%d")
                doi = draft.paper.doi or ""
                slug = doi.replace("/", "-").replace(".", "-").lower()[:40] if doi else "untitled"
                card_dir = os.path.join(card_news_base, f"{date_str}_{slug}")
                if os.path.isdir(card_dir):
                    drafts_with_dirs.append((draft_data, card_dir))
                else:
                    print(f"   âš ï¸ Card news not found: {card_dir}")

            await poster.post_batch(drafts_with_dirs)
        except Exception as e:
            print(f"âš ï¸ Instagram í¬ìŠ¤íŒ… ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")

    # Summary with source info
    print("\n" + "=" * 60)
    print("ğŸ“Š ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    for draft in drafts:
        status_icon = "âœ…" if draft.status == "ready_for_review" else "âš ï¸"
        source_tag = ""
        if draft.paper.topics:
            if "clinical_trial" in draft.paper.topics:
                source_tag = "[ì„ìƒì‹œí—˜] "
            elif "preprint" in draft.paper.topics:
                source_tag = "[í”„ë¦¬í”„ë¦°íŠ¸] "

        print(f"{status_icon} {source_tag}{draft.korean_title}")
        print(f"   ì‹ ë¢°ë„: {draft.confidence_score:.0%} | ìƒíƒœ: {draft.status}")
        print(f"   ì¶œì²˜: {draft.paper.journal}")

    return drafts


async def demo_multi_source():
    """Demo: Test multi-source paper discovery without content generation"""
    print("=" * 60)
    print("ğŸ”¬ ë©€í‹°ì†ŒìŠ¤ ë…¼ë¬¸ ê²€ìƒ‰ ë°ëª¨")
    print("=" * 60)

    discovery = PaperDiscovery()

    # Search specific topic across all sources
    query = "NAD+ longevity"
    print(f"\nğŸ” ê²€ìƒ‰ì–´: {query}")

    results = await discovery.search_all_sources(
        query=query,
        max_per_source=5,
        days_back=30,
        include_trials=True
    )

    print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼:")
    for source, items in results.items():
        print(f"\n--- {source.upper()} ({len(items)}ê°œ) ---")
        for item in items[:3]:  # Show top 3 per source
            if hasattr(item, 'title'):
                print(f"  â€¢ {item.title[:70]}...")
            elif hasattr(item, 'nct_id'):
                print(f"  â€¢ [{item.nct_id}] {item.title[:60]}...")


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "demo":
        asyncio.run(demo_multi_source())
    else:
        asyncio.run(main())
