#!/usr/bin/env python3
"""
ÏûêÎèôÌôîÎêú ÏΩòÌÖêÏ∏† ÌååÏù¥ÌîÑÎùºÏù∏
Automated Content Pipeline for Longevity Knowledge Platform

ÎÖºÎ¨∏ ÏàòÏßë ‚Üí AI ÏöîÏïΩ/Ìï¥ÏÑù ‚Üí Ìå©Ìä∏Ï≤¥ÌÅ¨ ‚Üí Î∞úÌñâ Ï§ÄÎπÑ

Components:
1. Paper Discovery (PubMed, RSS feeds)
2. Document Processing (Docling)
3. Content Generation (Gemini/GPT-4)
4. Fact Checking (GPT-4)
5. Publishing Preparation
"""

import os
import re
import json
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
from dotenv import load_dotenv


def parse_json_response(text: str) -> dict:
    """Parse JSON from AI response, stripping markdown code blocks if present"""
    # Strip ```json ... ``` or ``` ... ```
    cleaned = re.sub(r'^```(?:json)?\s*\n?', '', text.strip())
    cleaned = re.sub(r'\n?```\s*$', '', cleaned)
    try:
        return json.loads(cleaned)
    except json.JSONDecodeError:
        # Try to extract JSON object from mixed text
        match = re.search(r'\{[\s\S]*\}', cleaned)
        if match:
            return json.loads(match.group())
        raise

# Load .env file
load_dotenv()

# Configuration
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
KIMI_API_KEY = os.getenv("KIMI_API_KEY")  # Moonshot AI - much cheaper alternative


@dataclass
class Paper:
    """Research paper metadata"""
    title: str
    authors: List[str]
    abstract: str
    journal: str
    doi: str
    pub_date: str
    url: str
    full_text: Optional[str] = None
    relevance_score: float = 0.0
    topics: List[str] = None


@dataclass
class ContentDraft:
    """AI-generated content draft"""
    paper: Paper
    content_type: str  # newsletter, blog, youtube_script, vod_lecture
    korean_title: str
    korean_summary: str
    korean_body: str
    english_title: str
    english_summary: str
    key_insights: List[str]
    practical_applications: List[str]
    citations: List[Dict]
    fact_check_notes: List[str]
    confidence_score: float
    created_at: str
    status: str  # draft, reviewed, published
    source: str = ""  # pubmed, biorxiv, medrxiv, clinical_trial


@dataclass
class ClinicalTrial:
    """Clinical trial metadata"""
    nct_id: str
    title: str
    status: str
    phase: str
    conditions: List[str]
    interventions: List[str]
    summary: str
    start_date: str
    url: str
    relevance_score: float = 0.0


class PaperDiscovery:
    """
    ÌÜµÌï© ÎÖºÎ¨∏ Î∞úÍ≤¨ ÏãúÏä§ÌÖú
    Multi-source paper discovery: PubMed, bioRxiv, medRxiv, ClinicalTrials.gov

    Integrates with MCP servers:
    - pubmed: PubMed indexed papers
    - biorxiv: bioRxiv/medRxiv preprints
    - clinical-trials: ClinicalTrials.gov
    - scholar-gateway: Academic papers
    """

    LONGEVITY_KEYWORDS = [
        # Core longevity
        "NAD+ metabolism", "senolytics", "cellular senescence",
        "mitochondrial dysfunction", "autophagy aging",
        "longevity interventions", "healthspan", "lifespan extension",
        "metabolic aging", "epigenetic clock", "telomere attrition",
        # Therapeutics
        "GLP-1 agonist aging", "rapamycin longevity", "metformin aging",
        "NMN supplementation", "resveratrol", "spermidine",
        # Emerging
        "senostatics", "SASP inhibitors", "inflammaging",
        # Cancer diagnosis & therapeutics
        "liquid biopsy cancer", "immunotherapy checkpoint",
        "CAR-T cell therapy", "cancer biomarker early detection",
        "targeted therapy oncology", "tumor microenvironment",
        # Korean research focus
        "Korean longevity", "Asian metabolic disease",
    ]

    # Broader keywords for clinical trials (more general terms work better)
    CLINICAL_TRIAL_KEYWORDS = [
        "aging", "longevity", "senolytic", "NAD+", "NMN",
        "rapamycin", "metformin anti-aging", "GLP-1",
        "healthspan", "biological age", "caloric restriction",
        "nicotinamide riboside", "senolytics dasatinib quercetin",
        # Cancer
        "immunotherapy cancer", "CAR-T", "liquid biopsy",
        "checkpoint inhibitor", "targeted therapy cancer",
    ]

    RELEVANT_JOURNALS = [
        "Nature Aging", "Cell Metabolism", "Aging Cell",
        "GeroScience", "Lancet Healthy Longevity",
        "Nature Medicine", "Cell", "Nature", "Science",
        "Journal of Clinical Investigation", "JAMA",
        "bioRxiv", "medRxiv",  # Preprint servers
    ]

    def __init__(self):
        self.pubmed_base = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
        self.biorxiv_base = "https://api.biorxiv.org"
        self.clinicaltrials_base = "https://clinicaltrials.gov/api/v2"
        # Default retry settings for flaky networks (DNS/wifi sleep/wake, etc.)
        self.http_retries = int(os.getenv("DISCOVERY_HTTP_RETRIES", "4"))
        self.http_timeout_s = int(os.getenv("DISCOVERY_HTTP_TIMEOUT_S", "30"))

    # ============ PubMed Integration ============
    async def search_pubmed(
        self,
        query: str,
        max_results: int = 20,
        days_back: int = 7
    ) -> List[Paper]:
        """Search PubMed for recent papers"""
        import aiohttp

        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        date_filter = f"{start_date.strftime('%Y/%m/%d')}:{end_date.strftime('%Y/%m/%d')}[dp]"

        search_url = f"{self.pubmed_base}/esearch.fcgi"
        params = {
            "db": "pubmed",
            "term": f"({query}) AND {date_filter}",
            "retmax": max_results,
            "retmode": "json",
            "sort": "relevance"
        }

        ids: List[str] = []
        timeout = aiohttp.ClientTimeout(
            total=self.http_timeout_s,
            connect=min(10, self.http_timeout_s),
            sock_connect=min(10, self.http_timeout_s),
            sock_read=self.http_timeout_s,
        )
        for attempt in range(self.http_retries):
            try:
                async with aiohttp.ClientSession(timeout=timeout, trust_env=True) as session:
                    async with session.get(search_url, params=params) as resp:
                        if resp.status != 200:
                            return []
                        data = await resp.json()
                        ids = data.get("esearchresult", {}).get("idlist", [])
                break
            except (aiohttp.ClientError, OSError) as e:
                if attempt < self.http_retries - 1:
                    wait = min(60, 5 * (2 ** attempt))
                    print(f"PubMed search error (retry in {wait}s): {e}")
                    await asyncio.sleep(wait)
                    continue
                print(f"PubMed search failed (giving up): {e}")
                return []

        if not ids:
            return []

        return await self._fetch_pubmed_details(ids)

    async def _fetch_pubmed_details(self, pmids: List[str]) -> List[Paper]:
        """Fetch paper details from PubMed"""
        import aiohttp
        import xml.etree.ElementTree as ET

        fetch_url = f"{self.pubmed_base}/efetch.fcgi"
        params = {
            "db": "pubmed",
            "id": ",".join(pmids),
            "retmode": "xml"
        }

        papers = []
        xml_text = ""
        timeout = aiohttp.ClientTimeout(
            total=self.http_timeout_s,
            connect=min(10, self.http_timeout_s),
            sock_connect=min(10, self.http_timeout_s),
            sock_read=self.http_timeout_s,
        )
        for attempt in range(self.http_retries):
            try:
                async with aiohttp.ClientSession(timeout=timeout, trust_env=True) as session:
                    async with session.get(fetch_url, params=params) as resp:
                        if resp.status != 200:
                            return []
                        xml_text = await resp.text()
                break
            except (aiohttp.ClientError, OSError) as e:
                if attempt < self.http_retries - 1:
                    wait = min(60, 5 * (2 ** attempt))
                    print(f"PubMed fetch error (retry in {wait}s): {e}")
                    await asyncio.sleep(wait)
                    continue
                print(f"PubMed fetch failed (giving up): {e}")
                return []

        try:
            root = ET.fromstring(xml_text)
            for article in root.findall(".//PubmedArticle"):
                paper = self._parse_pubmed_article(article)
                if paper:
                    paper.topics = ["pubmed"]
                    papers.append(paper)
        except ET.ParseError:
            pass

        return papers

    def _parse_pubmed_article(self, article) -> Optional[Paper]:
        """Parse PubMed XML article"""
        try:
            medline = article.find(".//MedlineCitation")
            article_data = medline.find(".//Article")

            title = article_data.findtext(".//ArticleTitle", "")
            abstract = article_data.findtext(".//Abstract/AbstractText", "")

            authors = []
            for author in article_data.findall(".//Author"):
                last = author.findtext("LastName", "")
                first = author.findtext("ForeName", "")
                if last:
                    authors.append(f"{last} {first}".strip())

            journal = article_data.findtext(".//Journal/Title", "")
            pmid = medline.findtext(".//PMID", "")
            doi_elem = article_data.find(".//ELocationID[@EIdType='doi']")
            doi = doi_elem.text if doi_elem is not None else ""
            pub_date = medline.findtext(".//DateCompleted/Year", "")

            return Paper(
                title=title,
                authors=authors[:5],
                abstract=abstract,
                journal=journal,
                doi=doi,
                pub_date=pub_date,
                url=f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else "",
                topics=[]
            )
        except Exception:
            return None

    # ============ bioRxiv/medRxiv Integration ============
    async def search_biorxiv(
        self,
        query: str,
        max_results: int = 20,
        days_back: int = 30,
        server: str = "biorxiv"  # or "medrxiv"
    ) -> List[Paper]:
        """
        Search bioRxiv/medRxiv for preprints

        Note: bioRxiv API returns papers by date range, then we filter by query
        """
        import aiohttp

        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)

        # bioRxiv API endpoint for recent papers
        # Format: /details/[server]/[start_date]/[end_date]/[cursor]
        url = f"{self.biorxiv_base}/details/{server}/{start_date.strftime('%Y-%m-%d')}/{end_date.strftime('%Y-%m-%d')}/0/json"

        papers = []
        try:
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(
                    total=self.http_timeout_s,
                    connect=min(10, self.http_timeout_s),
                    sock_connect=min(10, self.http_timeout_s),
                    sock_read=self.http_timeout_s,
                ),
                trust_env=True,
            ) as session:
                # Paginate up to 3 pages (300 papers) for better coverage
                for cursor in range(0, 300, 100):
                    page_url = f"{self.biorxiv_base}/details/{server}/{start_date.strftime('%Y-%m-%d')}/{end_date.strftime('%Y-%m-%d')}/{cursor}/json"
                    async with session.get(page_url) as resp:
                        if resp.status != 200:
                            break
                        data = await resp.json()

                    collection = data.get("collection", [])
                    if not collection:
                        break

                    # Filter by query keywords (flexible word matching)
                    query_words = [w.lower().strip("+") for w in query.split() if len(w) >= 3]
                    for item in collection:
                        title = item.get("title", "").lower()
                        abstract = item.get("abstract", "").lower()
                        text = title + " " + abstract

                        # Match: ALL query words must appear (AND logic)
                        matches = sum(1 for w in query_words if w in text)
                        if query_words and matches >= len(query_words):
                            paper = Paper(
                                title=item.get("title", ""),
                                authors=item.get("authors", "").split("; ")[:5],
                                abstract=item.get("abstract", ""),
                                journal=f"{server} (preprint)",
                                doi=item.get("doi", ""),
                                pub_date=item.get("date", ""),
                                url=f"https://www.{server}.org/content/{item.get('doi', '')}",
                                topics=[server, "preprint"]
                            )
                            papers.append(paper)

                    if len(papers) >= max_results:
                        break

        except Exception as e:
            print(f"{server} search error: {e}")

        return papers[:max_results]

    async def search_medrxiv(
        self,
        query: str,
        max_results: int = 20,
        days_back: int = 30
    ) -> List[Paper]:
        """Search medRxiv for medical preprints"""
        return await self.search_biorxiv(query, max_results, days_back, server="medrxiv")

    # ============ ClinicalTrials.gov Integration ============
    async def search_clinical_trials(
        self,
        query: str,
        max_results: int = 20,
        status: str = "RECRUITING"  # RECRUITING, COMPLETED, ACTIVE_NOT_RECRUITING
    ) -> List[ClinicalTrial]:
        """
        Search ClinicalTrials.gov for relevant trials

        Useful for finding cutting-edge longevity interventions in human trials
        """
        import aiohttp

        url = f"{self.clinicaltrials_base}/studies"
        params = {
            "query.term": query,
            "filter.overallStatus": status,
            "pageSize": max_results,
            "format": "json"
        }

        trials = []
        try:
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(
                    total=self.http_timeout_s,
                    connect=min(10, self.http_timeout_s),
                    sock_connect=min(10, self.http_timeout_s),
                    sock_read=self.http_timeout_s,
                ),
                trust_env=True,
            ) as session:
                async with session.get(url, params=params) as resp:
                    if resp.status != 200:
                        return []
                    data = await resp.json()

            for study in data.get("studies", []):
                protocol = study.get("protocolSection", {})
                id_module = protocol.get("identificationModule", {})
                status_module = protocol.get("statusModule", {})
                desc_module = protocol.get("descriptionModule", {})
                conditions_module = protocol.get("conditionsModule", {})
                interventions_module = protocol.get("armsInterventionsModule", {})

                nct_id = id_module.get("nctId", "")

                trial = ClinicalTrial(
                    nct_id=nct_id,
                    title=id_module.get("briefTitle", ""),
                    status=status_module.get("overallStatus", ""),
                    phase=", ".join(protocol.get("designModule", {}).get("phases", [])),
                    conditions=conditions_module.get("conditions", []),
                    interventions=[
                        i.get("name", "") for i in
                        interventions_module.get("interventions", [])
                    ],
                    summary=desc_module.get("briefSummary", ""),
                    start_date=status_module.get("startDateStruct", {}).get("date", ""),
                    url=f"https://clinicaltrials.gov/study/{nct_id}"
                )
                trials.append(trial)

        except Exception as e:
            print(f"ClinicalTrials.gov search error: {e}")

        return trials

    def clinical_trial_to_paper(self, trial: ClinicalTrial) -> Paper:
        """Convert ClinicalTrial to Paper format for unified processing"""
        return Paper(
            title=f"[Clinical Trial] {trial.title}",
            authors=[],
            abstract=trial.summary,
            journal=f"ClinicalTrials.gov ({trial.phase})",
            doi=trial.nct_id,
            pub_date=trial.start_date,
            url=trial.url,
            topics=["clinical_trial"] + trial.conditions[:3]
        )

    # ============ Unified Multi-Source Search ============
    async def search_all_sources(
        self,
        query: str,
        max_per_source: int = 10,
        days_back: int = 14,
        include_trials: bool = True
    ) -> Dict[str, List]:
        """
        Search all sources in parallel

        Returns:
            {
                "pubmed": [Paper, ...],
                "biorxiv": [Paper, ...],
                "medrxiv": [Paper, ...],
                "clinical_trials": [ClinicalTrial, ...]
            }
        """
        tasks = [
            self.search_pubmed(query, max_per_source, days_back),
            self.search_biorxiv(query, max_per_source, days_back * 2),  # wider window for preprints
            self.search_medrxiv(query, max_per_source, days_back * 2),
        ]

        if include_trials:
            # Use simpler query for clinical trials (first significant word)
            trial_query = query.split()[0] if query else "longevity"
            tasks.append(self.search_clinical_trials(trial_query, max_per_source))

        results = await asyncio.gather(*tasks, return_exceptions=True)

        return {
            "pubmed": results[0] if not isinstance(results[0], Exception) else [],
            "biorxiv": results[1] if not isinstance(results[1], Exception) else [],
            "medrxiv": results[2] if not isinstance(results[2], Exception) else [],
            "clinical_trials": results[3] if len(results) > 3 and not isinstance(results[3], Exception) else []
        }

    async def get_weekly_papers(self, include_preprints: bool = True, include_trials: bool = True) -> List[Paper]:
        """
        Get this week's relevant papers from all sources

        Args:
            include_preprints: Include bioRxiv/medRxiv preprints
            include_trials: Include clinical trials (converted to Paper format)
        """
        all_papers = []
        print("üìö Searching multiple sources...")

        # Search each source for top keywords
        for keyword in self.LONGEVITY_KEYWORDS[:5]:
            print(f"   üîç Keyword: {keyword}")

            # PubMed (always)
            try:
                pubmed_papers = await self.search_pubmed(keyword, max_results=10, days_back=7)
            except Exception as e:
                print(f"PubMed keyword search failed (skipping): {e}")
                pubmed_papers = []
            for p in pubmed_papers:
                if not p.topics or "pubmed" not in p.topics:
                    p.topics = ["pubmed"]
            all_papers.extend(pubmed_papers)

            if include_preprints:
                # bioRxiv
                try:
                    biorxiv_papers = await self.search_biorxiv(keyword, max_results=5, days_back=30)
                except Exception as e:
                    print(f"biorxiv keyword search failed (skipping): {e}")
                    biorxiv_papers = []
                all_papers.extend(biorxiv_papers)

                # medRxiv (30 days, broader window for medical preprints)
                try:
                    medrxiv_papers = await self.search_medrxiv(keyword, max_results=5, days_back=30)
                except Exception as e:
                    print(f"medrxiv keyword search failed (skipping): {e}")
                    medrxiv_papers = []
                all_papers.extend(medrxiv_papers)

        # Clinical trials use broader keywords for better results
        if include_trials:
            for keyword in self.CLINICAL_TRIAL_KEYWORDS[:6]:
                print(f"   üè• Clinical trial keyword: {keyword}")
                try:
                    trials = await self.search_clinical_trials(keyword, max_results=5)
                except Exception as e:
                    print(f"ClinicalTrials keyword search failed (skipping): {e}")
                    trials = []
                for trial in trials:
                    all_papers.append(self.clinical_trial_to_paper(trial))

        # Deduplicate by DOI
        seen_dois = set()
        unique_papers = []
        for paper in all_papers:
            key = paper.doi or paper.title
            if key and key not in seen_dois:
                seen_dois.add(key)
                unique_papers.append(paper)

        # Score and rank
        for paper in unique_papers:
            paper.relevance_score = self._calculate_relevance(paper)

        print(f"   ‚úÖ Found {len(unique_papers)} unique papers")

        # Ensure source diversity: reserve slots for each source
        sorted_papers = sorted(unique_papers, key=lambda p: p.relevance_score, reverse=True)

        # Group by source
        by_source = {"pubmed": [], "biorxiv": [], "medrxiv": [], "clinical_trial": []}
        for paper in sorted_papers:
            src = "pubmed"
            for t in (paper.topics or []):
                if t in by_source:
                    src = t
                    break
            by_source[src].append(paper)

        # Guarantee minimum representation: 2 per non-empty source, fill rest by score
        selected = []
        used = set()
        for source in ["clinical_trial", "medrxiv", "biorxiv", "pubmed"]:
            for paper in by_source[source][:3]:  # up to 3 per source guaranteed
                key = paper.doi or paper.title
                if key not in used:
                    selected.append(paper)
                    used.add(key)

        # Fill remaining slots with highest-scored papers
        for paper in sorted_papers:
            if len(selected) >= 15:
                break
            key = paper.doi or paper.title
            if key not in used:
                selected.append(paper)
                used.add(key)

        return selected[:15]

    def _calculate_relevance(self, paper: Paper) -> float:
        """Calculate relevance score for a paper"""
        score = 0.0

        # Source bonus
        if paper.topics:
            if "clinical_trial" in paper.topics:
                score += 2.0  # Clinical trials are high value
            if "preprint" in paper.topics:
                score += 0.5  # Preprints are cutting edge

        # Journal impact
        if paper.journal:
            for journal in self.RELEVANT_JOURNALS:
                if journal.lower() in paper.journal.lower():
                    score += 3.0
                    break

        # Keyword matches in title
        title_lower = paper.title.lower()
        for keyword in self.LONGEVITY_KEYWORDS:
            if keyword.lower() in title_lower:
                score += 1.0

        # Abstract quality
        if paper.abstract and len(paper.abstract) > 500:
            score += 1.0

        return score


class DocumentProcessor:
    """
    Î¨∏ÏÑú Ï≤òÎ¶¨Í∏∞ (Docling Í∏∞Î∞ò)
    Process PDFs and documents using Docling
    """

    def __init__(self):
        self.docling_available = self._check_docling()

    def _check_docling(self) -> bool:
        """Check if docling is available"""
        try:
            from docling.document_converter import DocumentConverter
            return True
        except ImportError:
            return False

    async def process_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """Process a PDF file and extract structured content"""
        if not self.docling_available:
            return {"error": "Docling not installed. Run: pip install docling"}

        from docling.document_converter import DocumentConverter

        converter = DocumentConverter()
        result = converter.convert(pdf_path)
        doc = result.document

        return {
            "markdown": doc.export_to_markdown(),
            "text": doc.export_to_text(),
            "tables": [table.export_to_dataframe().to_dict() for table in doc.tables],
            "figures": [{"caption": fig.caption} for fig in doc.pictures],
            "metadata": {
                "title": doc.title,
                "pages": len(doc.pages) if hasattr(doc, 'pages') else 0
            }
        }


class ContentGenerator:
    """
    AI ÏΩòÌÖêÏ∏† ÏÉùÏÑ±Í∏∞
    Generates content using Gemini or GPT-4

    ÌïúÍµ≠Ïñ¥Î°ú Îî∞ÎúªÌïòÍ≥† Ï†ÑÎ¨∏Ï†ÅÏù∏ ÏΩòÌÖêÏ∏† ÏÉùÏÑ±
    """

    SYSTEM_PROMPT_KOREAN = """ÎãπÏã†ÏùÄ Î∏åÎùºÏö¥Î∞îÏù¥Ïò§ÌÖç(Brown Biotech)Ïùò Ïû•ÏàòÍ≥ºÌïô Î¶¨ÏÑúÏπòÌåÄ Í¥ÄÏ†êÏúºÎ°ú Í∏ÄÏùÑ ÏûëÏÑ±ÌïòÎäî AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§.

Ïó≠Ìï†:
- ÏµúÏã† ÏùòÌïô Ïó∞Íµ¨Î•º ÎåÄÏ§ëÏù¥ Ïù¥Ìï¥ÌïòÍ∏∞ ÏâΩÍ≤å ÏÑ§Î™Ö
- Í≥ºÌïôÏ†Å Ï†ïÌôïÏÑ±ÏùÑ Ïú†ÏßÄÌïòÎ©¥ÏÑúÎèÑ Îî∞ÎúªÌïòÍ≥† Í≥µÍ∞êÏ†ÅÏù∏ ÌÜ§ Ïú†ÏßÄ
- Ïã§Ïö©Ï†ÅÏù∏ Í±¥Í∞ï Ïù∏ÏÇ¨Ïù¥Ìä∏ Ï†úÍ≥µ

Í∏ÄÏì∞Í∏∞ Ïä§ÌÉÄÏùº:
- Ï†ÑÎ¨∏ Ïö©Ïñ¥Îäî Ïâ¨Ïö¥ ÏÑ§Î™ÖÍ≥º Ìï®Íªò ÏÇ¨Ïö©
- ÎèÖÏûêÏôÄ ÎåÄÌôîÌïòÎìØ ÏπúÍ∑ºÌïòÍ≤å (Ï°¥ÎåìÎßê ÏÇ¨Ïö©)
- ÌïµÏã¨ Î©îÏãúÏßÄÎ•º Î™ÖÌôïÌïòÍ≤å
- Ìù¨ÎßùÏ†ÅÏù¥Í≥† Í∏çÏ†ïÏ†ÅÏù∏ ÌÜ§ Ïú†ÏßÄ

Ï£ºÏùòÏÇ¨Ìï≠:
- ÏùòÎ£åÏ†Å Ï°∞Ïñ∏ÏùÄ "~Ìï† Ïàò ÏûàÏäµÎãàÎã§", "~ÎùºÍ≥† Ìï©ÎãàÎã§" ÌòïÏãùÏúºÎ°ú
- Ï∂úÏ≤òÎ•º Î™ÖÌôïÌûà Î∞ùÌûàÍ∏∞
- Í≥ºÏû•ÌïòÍ±∞ÎÇò ÌôïÏ†ïÏ†ÅÏù∏ ÌëúÌòÑ ÏûêÏ†ú"""

    CONTENT_TEMPLATES = {
        "newsletter": """
## Îâ¥Ïä§Î†àÌÑ∞ ÌòïÏãù

Ï†úÎ™©: [Ìù•ÎØ∏Î•º ÎÅÑÎäî ÏßàÎ¨∏ ÌòïÏãùÏùò Ï†úÎ™©]

Ïù∏ÏÇ¨Îßê:
ÏïàÎÖïÌïòÏÑ∏Ïöî, Î∏åÎùºÏö¥Î∞îÏù¥Ïò§ÌÖçÏûÖÎãàÎã§.
Ïù¥Î≤à Ï£ºÏóê Ï£ºÎ™©Ìï† ÎßåÌïú Ïó∞Íµ¨Í∞Ä Î∞úÌëúÎêòÏóàÏäµÎãàÎã§.

ÌïµÏã¨ ÎÇ¥Ïö© (3-4 Î¨∏Îã®):
- Ïó∞Íµ¨ Î∞∞Í≤ΩÍ≥º Ï§ëÏöîÏÑ±
- Ï£ºÏöî Î∞úÍ≤¨ ÎÇ¥Ïö©
- Ïö∞Î¶¨ Í±¥Í∞ïÏóê ÎØ∏ÏπòÎäî ÏùòÎØ∏
- Ïã§Ï≤úÌï† Ïàò ÏûàÎäî Ï†ê

ÎßàÎ¨¥Î¶¨:
Í±¥Í∞ïÌïú Ìïú Ï£º Î≥¥ÎÇ¥ÏãúÍ∏∞ Î∞îÎûçÎãàÎã§.

Ï¥ù Í∏∏Ïù¥: 400-600Ïûê
""",
        "blog": """
## Î∏îÎ°úÍ∑∏ ÌòïÏãù

Ï†úÎ™©: [SEO ÏµúÏ†ÅÌôîÎêú Ï†úÎ™©]
Î∂ÄÏ†ú: [ÎÇ¥Ïö©ÏùÑ ÏöîÏïΩÌïòÎäî Ìïú Ï§Ñ]

ÎèÑÏûÖÎ∂Ä:
- ÎèÖÏûêÏùò Í¥ÄÏã¨ÏùÑ ÎÅÑÎäî ÏßàÎ¨∏Ïù¥ÎÇò ÏÉÅÌô© Ï†úÏãú
- Ïù¥ Ïó∞Íµ¨Í∞Ä Ïôú Ï§ëÏöîÌïúÏßÄ

Î≥∏Î¨∏ (5-7 Î¨∏Îã®):
1. Ïó∞Íµ¨ Î∞∞Í≤Ω
2. Î∞©Î≤ïÎ°† (Í∞ÑÎûµÌûà)
3. Ï£ºÏöî Í≤∞Í≥º
4. Ï†ÑÎ¨∏Í∞Ä Ìï¥ÏÑù
5. ÌïúÍ≥ÑÏ†ê (Í∑†Ìòï Ïû°Ìûå ÏãúÍ∞Å)
6. Ïã§ÏÉùÌôú Ï†ÅÏö©

Í≤∞Î°†:
- ÌïµÏã¨ Î©îÏãúÏßÄ ÏöîÏïΩ
- ÎèÖÏûê ÌñâÎèô Ïú†ÎèÑ (CTA)

Ï¥ù Í∏∏Ïù¥: 800-1200Ïûê
""",
        "youtube_script": """
## Ïú†ÌäúÎ∏å Ïä§ÌÅ¨Î¶ΩÌä∏ ÌòïÏãù

[Ïò§ÌîÑÎãù - 10Ï¥à]
"Ïó¨Îü¨Î∂Ñ, Ïò§Îäò Î∞úÌëúÎêú Ï∂©Í≤©Ï†ÅÏù∏ Ïó∞Íµ¨ Í≤∞Í≥ºÍ∞Ä ÏûàÏäµÎãàÎã§..."

[ÌïµÏã¨ ÏßàÎ¨∏ - 20Ï¥à]
"[Ïó∞Íµ¨ Ï£ºÏ†ú]Í∞Ä Ï†ïÎßê [Ìö®Í≥º]Ïóê ÎèÑÏõÄÏù¥ Îê†ÍπåÏöî?"

[Ïó∞Íµ¨ ÏÜåÍ∞ú - 1Î∂Ñ]
- Ïñ¥ÎîîÏÑú, ÎàÑÍ∞Ä, Ïñ¥ÎñªÍ≤å Ïó∞Íµ¨ÌñàÎäîÏßÄ
- Ïôú Ïù¥ Ïó∞Íµ¨Í∞Ä Ï§ëÏöîÌïúÏßÄ

[Í≤∞Í≥º ÏÑ§Î™Ö - 2Î∂Ñ]
- ÌïµÏã¨ Î∞úÍ≤¨ 1, 2, 3
- Í∑∏ÎûòÌîÑ/Ìëú ÏÑ§Î™Ö ÏãúÏ†ê ÌëúÏãú [ÏûêÎßâ: ...]

[ÏùòÎØ∏ Ìï¥ÏÑù - 1Î∂Ñ]
- Ïö∞Î¶¨ÏóêÍ≤å Ïñ¥Îñ§ ÏùòÎØ∏Ïù∏ÏßÄ
- Ï£ºÏùòÌï† Ï†ê

[Ïã§Ï≤ú ÌåÅ - 1Î∂Ñ]
- Ïò§ÎäòÎ∂ÄÌÑ∞ Ìï† Ïàò ÏûàÎäî Í≤É

[ÌÅ¥Î°úÏßï - 30Ï¥à]
- Íµ¨ÎèÖ/Ï¢ãÏïÑÏöî ÏöîÏ≤≠
- Îã§Ïùå ÏòÅÏÉÅ ÏòàÍ≥†

Ï¥ù Í∏∏Ïù¥: 5-7Î∂Ñ Î∂ÑÎüâ
"""
    }

    def __init__(self, provider: str = "gemini"):
        """
        Initialize content generator

        Args:
            provider: "gemini", "openai", or "kimi"
        """
        self.provider = provider

    def _provider_has_key(self, provider: str) -> bool:
        if provider == "kimi":
            return bool(KIMI_API_KEY)
        if provider == "gemini":
            return bool(GEMINI_API_KEY)
        if provider == "openai":
            return bool(OPENAI_API_KEY)
        return False

    def _fallback_provider(self) -> Optional[str]:
        """
        Choose a reasonable fallback provider when the primary provider errors.
        Preference: kimi -> gemini -> openai (cost first), but only if keys exist.
        """
        for p in ("kimi", "gemini", "openai"):
            if p != self.provider and self._provider_has_key(p):
                return p
        return None

    async def generate_content(
        self,
        paper: Paper,
        content_type: str = "newsletter",
        language: str = "korean"
    ) -> ContentDraft:
        """Generate content from a research paper"""

        template = self.CONTENT_TEMPLATES.get(content_type, self.CONTENT_TEMPLATES["newsletter"])

        prompt = f"""Îã§Ïùå Ïó∞Íµ¨ ÎÖºÎ¨∏ÏùÑ Î∞îÌÉïÏúºÎ°ú {content_type} ÏΩòÌÖêÏ∏†Î•º ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.

## ÎÖºÎ¨∏ Ï†ïÎ≥¥
Ï†úÎ™©: {paper.title}
Ï†ÄÏûê: {', '.join(paper.authors)}
Ï†ÄÎÑê: {paper.journal}
Î∞úÌñâÏùº: {paper.pub_date}

## Ï¥àÎ°ù
{paper.abstract}

## ÏûëÏÑ± ÌòïÏãù
{template}

## Ï∂îÍ∞Ä ÏöîÏ≤≠ÏÇ¨Ìï≠
1. ÌïúÍµ≠Ïñ¥Î°ú ÏûëÏÑ±
2. Ï†ÑÎ¨∏ Ïö©Ïñ¥Îäî ÏòÅÏñ¥ ÏõêÎ¨∏ÏùÑ Í¥ÑÌò∏Î°ú Î≥ëÍ∏∞
3. ÌïµÏã¨ Ïù∏ÏÇ¨Ïù¥Ìä∏ 3Í∞ÄÏßÄ Î≥ÑÎèÑ Ï†ïÎ¶¨
4. Ïã§Ïö©Ï†Å Ï†ÅÏö© Î∞©Î≤ï 2-3Í∞ÄÏßÄ Ï†úÏïà
5. Ïù∏Ïö© Ï∂úÏ≤ò Î™ÖÏãú

JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî:
{{
    "korean_title": "Ï†úÎ™©",
    "korean_summary": "2-3Î¨∏Ïû• ÏöîÏïΩ",
    "korean_body": "Î≥∏Î¨∏ Ï†ÑÏ≤¥",
    "key_insights": ["Ïù∏ÏÇ¨Ïù¥Ìä∏1", "Ïù∏ÏÇ¨Ïù¥Ìä∏2", "Ïù∏ÏÇ¨Ïù¥Ìä∏3"],
    "practical_applications": ["Ï†ÅÏö©1", "Ï†ÅÏö©2"],
    "confidence_score": 0.0-1.0
}}"""

        async def call_with_provider(provider: str) -> str:
            if provider == "gemini":
                return await self._call_gemini(prompt)
            if provider == "kimi":
                return await self._call_kimi(prompt)
            return await self._call_openai(prompt)

        response: str
        try:
            response = await call_with_provider(self.provider)
        except Exception:
            fb = self._fallback_provider()
            if not fb:
                raise
            response = await call_with_provider(fb)

        try:
            content_data = parse_json_response(response)
        except json.JSONDecodeError:
            # Treat malformed output as a hard failure so we can try another paper/provider.
            raise ValueError("AI response was not valid JSON")

        # Determine source from paper topics
        source = "pubmed"  # default
        if paper.topics:
            if "clinical_trial" in paper.topics:
                source = "clinical_trial"
            elif "medrxiv" in paper.topics:
                source = "medrxiv"
            elif "biorxiv" in paper.topics:
                source = "biorxiv"

        return ContentDraft(
            paper=paper,
            content_type=content_type,
            korean_title=content_data.get("korean_title", ""),
            korean_summary=content_data.get("korean_summary", ""),
            korean_body=content_data.get("korean_body", ""),
            english_title=paper.title,
            english_summary=paper.abstract,  # Full abstract, not truncated
            key_insights=content_data.get("key_insights", []),
            practical_applications=content_data.get("practical_applications", []),
            citations=[{"doi": paper.doi, "title": paper.title, "journal": paper.journal}],
            fact_check_notes=[],
            confidence_score=content_data.get("confidence_score", 0.5),
            created_at=datetime.now().isoformat(),
            status="draft",
            source=source,
        )

    async def _call_gemini(self, prompt: str, max_retries: int = 3) -> str:
        """Call Gemini API with retry on rate limit (new google.genai SDK)"""
        from google import genai
        from google.genai import types

        client = genai.Client(api_key=GEMINI_API_KEY)

        for attempt in range(max_retries):
            try:
                response = client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=[self.SYSTEM_PROMPT_KOREAN + "\n\n" + prompt],
                    config=types.GenerateContentConfig(
                        temperature=0.7,
                        max_output_tokens=8192,
                        safety_settings=[
                            types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                            types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                            types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                            types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                        ],
                    ),
                )
                if not getattr(response, "text", None):
                    raise RuntimeError("Gemini returned empty response")
                return response.text
            except Exception as e:
                error_str = str(e)
                if "429" in error_str and attempt < max_retries - 1:
                    wait = (attempt + 1) * 15
                    print(f"   ‚è≥ Rate limit, {wait}Ï¥à ÎåÄÍ∏∞ ÌõÑ Ïû¨ÏãúÎèÑ ({attempt + 1}/{max_retries})...")
                    await asyncio.sleep(wait)
                    continue
                raise RuntimeError(f"Gemini API error: {e}") from e

    async def _call_openai(self, prompt: str) -> str:
        """Call OpenAI API"""
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=OPENAI_API_KEY)

        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": self.SYSTEM_PROMPT_KOREAN},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=2048
        )
        content = response.choices[0].message.content
        if not content:
            raise RuntimeError("OpenAI returned empty response")
        return content

    async def _call_kimi(self, prompt: str) -> str:
        """
        Call Kimi (Moonshot AI) API - Ï†ÄÎ†¥Ìïú ÎåÄÏïà

        Pricing (2026):
        - Input: $0.45-0.60 / 1M tokens
        - Output: $2.50 / 1M tokens
        - 75% discount with caching

        vs OpenAI GPT-4o:
        - Input: $2.50 / 1M tokens
        - Output: $10.00 / 1M tokens
        """
        from openai import AsyncOpenAI

        # Kimi uses OpenAI-compatible API
        client = AsyncOpenAI(
            api_key=KIMI_API_KEY,
            base_url="https://api.moonshot.ai/v1",
            timeout=120.0  # Kimi can be slower, allow 2 min
        )

        response = await client.chat.completions.create(
            model="moonshot-v1-8k",  # Use 8k for faster response
            messages=[
                {"role": "system", "content": self.SYSTEM_PROMPT_KOREAN},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=2048
        )
        content = response.choices[0].message.content
        if not content:
            raise RuntimeError("Kimi returned empty response")
        return content

    async def revise_content(
        self,
        paper: Paper,
        draft: ContentDraft,
        issues: List[str],
        content_type: str = "newsletter",
    ) -> ContentDraft:
        """
        Try to auto-fix a draft based on fact-check issues.
        This is intentionally conservative: prefer removing/qualifying claims over inventing details.
        """
        issues_text = "\n".join(f"- {i}" for i in (issues or [])[:8])
        prompt = f"""Îã§Ïùå ÏΩòÌÖêÏ∏†Î•º ÏõêÎ≥∏ Ï¥àÎ°ù Î≤îÏúÑ ÎÇ¥ÏóêÏÑú ÏàòÏ†ïÌï¥Ï£ºÏÑ∏Ïöî.

Ï§ëÏöî:
- Ï¥àÎ°ù/Ï†úÎ™©Ïóê ÏóÜÎäî Ïà´Ïûê, Ï†ÄÏûê, Ïó∞ÎèÑ, Í≤∞Í≥ºÎ•º ÏÉàÎ°ú ÎßåÎì§ÏßÄ ÎßàÏÑ∏Ïöî.
- ÌôïÏã§ÌïòÏßÄ ÏïäÏúºÎ©¥ 'Ï¥àÎ°ùÏóêÏÑú ÌôïÏù∏ÎêòÏßÄ ÏïäÏäµÎãàÎã§'ÎùºÍ≥† Î™ÖÏãúÌïòÏÑ∏Ïöî.
- Ïö©Ïñ¥Îäî ÏõêÎ¨∏ ÏùòÎØ∏Î•º Ïú†ÏßÄÌïòÏÑ∏Ïöî(Ïòà: knockout vs inhibition Îì±).
- Ï∂úÎ†•ÏùÄ Î∞òÎìúÏãú JSONÎßå(ÏΩîÎìúÎ∏îÎ°ù/ÏÑ§Î™Ö ÏóÜÏù¥) Î∞òÌôòÌïòÏÑ∏Ïöî.

## ÏõêÎ≥∏ ÎÖºÎ¨∏
Ï†úÎ™©: {paper.title}
Ï†ÄÏûê: {', '.join(paper.authors)}
Ï†ÄÎÑê: {paper.journal}
Î∞úÌñâÏùº: {paper.pub_date}

Ï¥àÎ°ù:
{paper.abstract}

## Í∏∞Ï°¥ ÏΩòÌÖêÏ∏†(JSON ÏùºÎ∂Ä)
korean_title: {draft.korean_title}
korean_summary: {draft.korean_summary}
korean_body:
{draft.korean_body}

## Ìå©Ìä∏Ï≤¥ÌÅ¨ Ïù¥Ïäà
{issues_text}

JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµ:
{{
  "korean_title": "...",
  "korean_summary": "...",
  "korean_body": "...",
  "key_insights": ["...", "...", "..."],
  "practical_applications": ["...", "..."],
  "confidence_score": 0.0-1.0
}}"""

        original_provider = self.provider
        try:
            response = await (self._call_gemini(prompt) if self.provider == "gemini"
                              else self._call_kimi(prompt) if self.provider == "kimi"
                              else self._call_openai(prompt))
        except Exception:
            fb = self._fallback_provider()
            if not fb:
                raise
            self.provider = fb
            response = await (self._call_gemini(prompt) if self.provider == "gemini"
                              else self._call_kimi(prompt) if self.provider == "kimi"
                              else self._call_openai(prompt))
        finally:
            self.provider = original_provider

        content_data = parse_json_response(response)
        draft.korean_title = content_data.get("korean_title", draft.korean_title)
        draft.korean_summary = content_data.get("korean_summary", draft.korean_summary)
        draft.korean_body = content_data.get("korean_body", draft.korean_body)
        draft.key_insights = content_data.get("key_insights", draft.key_insights)
        draft.practical_applications = content_data.get("practical_applications", draft.practical_applications)
        draft.confidence_score = content_data.get("confidence_score", draft.confidence_score)
        return draft


class FactChecker:
    """
    Ìå©Ìä∏ Ï≤¥Ïª§
    Validates content accuracy using GPT-4 or Kimi (cheaper)
    """

    def __init__(self, provider: str = "gemini"):
        """
        Initialize fact checker

        Args:
            provider: "gemini", "openai", or "kimi"
        """
        self.provider = provider

    FACT_CHECK_PROMPT = """ÎãπÏã†ÏùÄ ÏùòÌïô ÎÖºÎ¨∏ Ìå©Ìä∏Ï≤¥Ïª§ÏûÖÎãàÎã§.

Îã§Ïùå AI ÏÉùÏÑ± ÏΩòÌÖêÏ∏†Í∞Ä Ï†úÍ≥µÎêú "ÎÖºÎ¨∏ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ + Ï¥àÎ°ù"Í≥º ÏùºÏπòÌïòÎäîÏßÄ Í≤ÄÏ¶ùÌï¥Ï£ºÏÑ∏Ïöî.

Ï£ºÏùò:
- Ï¥àÎ°ùÏóê ÏóÜÎäî ÎÇ¥Ïö©Ïù¥ÎùºÎèÑ, ÏïÑÎûò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞(Ï†ÄÏûê/Ï†ÄÎÑê/Ïó∞ÎèÑ/DOI/URL)Îäî ÏÇ¨Ïã§Î°ú Í∞ÑÏ£ºÌï† Ïàò ÏûàÏäµÎãàÎã§.
- Ïù∏Ïö© ÌòïÏãù(Ïòà: et al.)Ïùò Ïä§ÌÉÄÏùº Ï∞®Ïù¥ÎßåÏúºÎ°úÎäî Î¨∏Ï†úÎ°ú ÏÇºÏßÄ ÎßàÏÑ∏Ïöî.
- Î≥∏Î¨∏Ïù¥ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞/Ï¥àÎ°ùÏùÑ ÎÑòÏñ¥ÏÑúÎäî 'Ï∂îÍ∞Ä Ïà´Ïûê/Í≤∞Í≥º/Ïù∏Í≥º'Î•º Ï†úÏãúÌïòÎ©¥ Î¨∏Ï†úÎ°ú ÏßÄÏ†ÅÌïòÏÑ∏Ïöî.
- ÌôïÏã§ÌïòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ 'Ï¥àÎ°ùÏóêÏÑú ÌôïÏù∏ÎêòÏßÄ ÏïäÏäµÎãàÎã§'Î°ú ÌëúÏãúÌïòÎèÑÎ°ù Ï†úÏïàÌïòÏÑ∏Ïöî.

## ÏõêÎ≥∏ ÎÖºÎ¨∏(Î©îÌÉÄÎç∞Ïù¥ÌÑ∞)
Ï†úÎ™©: {title}
Ï†ÄÏûê: {authors}
Ï†ÄÎÑê: {journal}
Î∞úÌñâÏùº: {pub_date}
DOI: {doi}
URL: {url}

## ÏõêÎ≥∏ Ï¥àÎ°ù
{abstract}

## AI ÏÉùÏÑ± ÏΩòÌÖêÏ∏†
{content}

## Í≤ÄÏ¶ù Ìï≠Î™©
1. Ïà´Ïûê/ÌÜµÍ≥Ñ Ï†ïÌôïÏÑ± (Ï¥àÎ°ù/Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò)
2. Ïù∏Í≥ºÍ¥ÄÍ≥Ñ ÏôúÍ≥° Ïó¨Î∂Ä
3. Í≥ºÏû•Îêú ÌëúÌòÑ Ïó¨Î∂Ä
4. Ïû†Ïû¨Ï†Å Ïò§Ìï¥ Í∞ÄÎä•ÏÑ±

JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµ:
{{
  "accuracy_score": 0.0-1.0,
  "issues": ["Î¨∏Ï†úÏ†ê1", "Î¨∏Ï†úÏ†ê2"],
  "suggestions": ["ÏàòÏ†ïÏ†úÏïà1", "ÏàòÏ†ïÏ†úÏïà2"],
  "safe_to_publish": true/false
}}"""

    async def check(self, draft: ContentDraft) -> Dict[str, Any]:
        """Fact-check a content draft"""
        prompt = self.FACT_CHECK_PROMPT.format(
            title=draft.paper.title,
            authors=", ".join(draft.paper.authors or []),
            journal=draft.paper.journal,
            pub_date=draft.paper.pub_date,
            doi=draft.paper.doi,
            url=draft.paper.url,
            abstract=draft.paper.abstract,
            content=draft.korean_body
        )

        async def try_provider(provider: str) -> Dict[str, Any]:
            if provider == "kimi":
                return await self._check_with_kimi(prompt)
            if provider == "gemini":
                return await self._check_with_gemini(prompt)
            return await self._check_with_openai(prompt)

        # First attempt with configured provider; if it fails due to auth/timeout/parse,
        # fall back to another provider that has a key configured.
        primary = self.provider
        result = await try_provider(primary)

        issues_text = " ".join(result.get("issues", []) or [])
        should_fallback = (
            result.get("accuracy_score", 0.0) == 0.0
            and ("Invalid Authentication" in issues_text or "401" in issues_text or "timed out" in issues_text)
        ) or ("JSON ÌååÏã± Ïã§Ìå®" in issues_text)

        if not should_fallback:
            return result

        fallbacks: List[str] = []
        # Prefer OpenAI for fact-check if available, then Gemini, then Kimi.
        if primary != "openai" and OPENAI_API_KEY:
            fallbacks.append("openai")
        if primary != "gemini" and GEMINI_API_KEY:
            fallbacks.append("gemini")
        if primary != "kimi" and KIMI_API_KEY:
            fallbacks.append("kimi")

        for fb in fallbacks:
            fb_result = await try_provider(fb)
            fb_issues_text = " ".join(fb_result.get("issues", []) or [])
            # Accept fallback if it produced a non-zero score or a publishable result.
            if fb_result.get("safe_to_publish") or fb_result.get("accuracy_score", 0.0) > 0.0:
                return fb_result
            # If fallback isn't an auth/timeout failure, return it (more informative).
            if "Invalid Authentication" not in fb_issues_text and "401" not in fb_issues_text:
                return fb_result

        return result

    async def _check_with_gemini(self, prompt: str) -> Dict[str, Any]:
        """Fact-check using Gemini (new google.genai SDK)"""
        try:
            from google import genai
            from google.genai import types

            client = genai.Client(api_key=GEMINI_API_KEY)

            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=[prompt],
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    max_output_tokens=4096,
                    safety_settings=[
                        types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                    ],
                ),
            )
            return parse_json_response(response.text)
        except json.JSONDecodeError:
            return {
                "accuracy_score": 0.7,
                "issues": ["JSON ÌååÏã± Ïã§Ìå® - ÏàòÎèô Í≤ÄÌÜ† ÌïÑÏöî"],
                "suggestions": [],
                "safe_to_publish": False
            }
        except Exception as e:
            return {
                "accuracy_score": 0.0,
                "issues": [f"Fact check failed (Gemini): {e}"],
                "suggestions": [],
                "safe_to_publish": False
            }

    async def _check_with_openai(self, prompt: str) -> Dict[str, Any]:
        """Fact-check using OpenAI GPT-4"""
        try:
            from openai import AsyncOpenAI

            client = AsyncOpenAI(api_key=OPENAI_API_KEY)

            response = await client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1024
            )

            return parse_json_response(response.choices[0].message.content)
        except Exception as e:
            return {
                "accuracy_score": 0.0,
                "issues": [f"Fact check failed: {e}"],
                "suggestions": [],
                "safe_to_publish": False
            }

    async def _check_with_kimi(self, prompt: str) -> Dict[str, Any]:
        """Fact-check using Kimi (cheaper alternative)"""
        try:
            from openai import AsyncOpenAI

            client = AsyncOpenAI(
                api_key=KIMI_API_KEY,
                base_url="https://api.moonshot.ai/v1",
                timeout=120.0
            )

            response = await client.chat.completions.create(
                model="moonshot-v1-8k",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1024
            )

            return parse_json_response(response.choices[0].message.content)
        except Exception as e:
            return {
                "accuracy_score": 0.0,
                "issues": [f"Fact check failed (Kimi): {e}"],
                "suggestions": [],
                "safe_to_publish": False
            }


class ContentPipeline:
    """
    Ï†ÑÏ≤¥ ÏΩòÌÖêÏ∏† ÌååÏù¥ÌîÑÎùºÏù∏
    Orchestrates the entire content generation workflow

    Provider options:
    - "gemini": Google Gemini (default, moderate cost)
    - "openai": OpenAI GPT-4 (highest quality, highest cost)
    - "kimi": Moonshot Kimi (good quality, lowest cost - recommended for budget)
    """

    def __init__(self, ai_provider: str = "gemini", fact_check_provider: str = None):
        """
        Initialize content pipeline

        Args:
            ai_provider: Provider for content generation ("gemini", "openai", "kimi")
            fact_check_provider: Provider for fact checking (defaults to ai_provider)
        """
        self.discovery = PaperDiscovery()
        self.doc_processor = DocumentProcessor()
        self.generator = ContentGenerator(provider=ai_provider)
        # Use same provider for fact-check by default
        fc_provider = fact_check_provider or ai_provider
        self.fact_checker = FactChecker(provider=fc_provider)

    async def run_weekly_pipeline(
        self,
        include_preprints: bool = True,
        include_trials: bool = True
    ) -> List[ContentDraft]:
        """
        Ï£ºÍ∞Ñ ÏΩòÌÖêÏ∏† ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ (Î©ÄÌã∞ÏÜåÏä§)

        Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§:
        - PubMed: ÌîºÏñ¥Î¶¨Î∑∞ ÎÖºÎ¨∏
        - bioRxiv/medRxiv: ÌîÑÎ¶¨ÌîÑÎ¶∞Ìä∏ (ÏµúÏã† Ïó∞Íµ¨)
        - ClinicalTrials.gov: ÏßÑÌñâ Ï§ëÏù∏ ÏûÑÏÉÅÏãúÌóò

        ÌååÏù¥ÌîÑÎùºÏù∏:
        1. Î©ÄÌã∞ÏÜåÏä§ ÎÖºÎ¨∏ ÏàòÏßë
        2. ÏÉÅÏúÑ 5Í∞ú ÏÑ†Ï†ï
        3. Í∞Å ÎÖºÎ¨∏Î≥Ñ Îâ¥Ïä§Î†àÌÑ∞ ÏΩòÌÖêÏ∏† ÏÉùÏÑ±
        4. Ìå©Ìä∏Ï≤¥ÌÅ¨
        5. Í≤ÄÌÜ† ÎåÄÍ∏∞ ÏÉÅÌÉúÎ°ú Ï†ÄÏû•
        """
        print("üîç Î©ÄÌã∞ÏÜåÏä§ ÎÖºÎ¨∏ ÏàòÏßë Ï§ë...")
        print("   üìö Sources: PubMed", end="")
        if include_preprints:
            print(" + bioRxiv + medRxiv", end="")
        if include_trials:
            print(" + ClinicalTrials.gov", end="")
        print()

        papers = await self.discovery.get_weekly_papers(
            include_preprints=include_preprints,
            include_trials=include_trials
        )

        # Show source breakdown
        source_counts = {"pubmed": 0, "biorxiv": 0, "medrxiv": 0, "clinical_trial": 0}
        for paper in papers:
            for topic in (paper.topics or []):
                if topic in source_counts:
                    source_counts[topic] += 1
                    break  # count each paper once

        print(f"\nüìä ÏàòÏßë Í≤∞Í≥º:")
        for source, count in source_counts.items():
            icon = {"pubmed": "üìÑ", "biorxiv": "üß¨", "medrxiv": "üè•", "clinical_trial": "üíä"}.get(source, "‚Ä¢")
            print(f"   {icon} {source}: {count}Í∞ú")
        print(f"   Ï¥ù {len(papers)}Í∞ú ÎÖºÎ¨∏ Î∞úÍ≤¨")

        # Target number of publishable drafts. If a draft can't be auto-fixed, try the next paper.
        target_ready = int(os.getenv("TARGET_READY_DRAFTS", "5"))
        max_papers = int(os.getenv("MAX_PAPERS_TO_PROCESS", "15"))
        max_revisions = int(os.getenv("MAX_AUTO_REVISIONS", "2"))

        drafts: List[ContentDraft] = []
        ready_count = 0
        processed = 0

        for paper in papers:
            if processed >= max_papers or ready_count >= target_ready:
                break
            processed += 1

            print(f"\nüìù ÏΩòÌÖêÏ∏† ÏÉùÏÑ± Ï§ë ({ready_count + 1}/{target_ready}): {paper.title[:50]}...")

            try:
                draft = await self.generator.generate_content(
                    paper,
                    content_type="newsletter",
                    language="korean"
                )
            except Exception as e:
                print(f"   ‚ö†Ô∏è ÏÉùÏÑ± Ïã§Ìå®(Í±¥ÎÑàÎúÄ): {e}")
                continue

            # Fact check + auto-revise loop
            for attempt in range(max_revisions + 1):
                print(f"   ‚úì Ìå©Ìä∏Ï≤¥ÌÅ¨ Ï§ë...")
                fact_result = await self.fact_checker.check(draft)
                draft.fact_check_notes = fact_result.get("issues", [])

                if fact_result.get("safe_to_publish", False):
                    draft.status = "ready_for_review"
                    print(f"   ‚úÖ Í≤ÄÌÜ† Ï§ÄÎπÑ ÏôÑÎ£å (Ï†ïÌôïÎèÑ: {fact_result.get('accuracy_score', 0):.0%})")
                    break

                # Auto-revise if we still have attempts left
                if attempt < max_revisions and draft.fact_check_notes:
                    print(f"   üîÅ ÏûêÎèô ÏàòÏ†ï ÏãúÎèÑ ({attempt + 1}/{max_revisions})...")
                    try:
                        draft = await self.generator.revise_content(
                            paper=paper,
                            draft=draft,
                            issues=draft.fact_check_notes,
                            content_type="newsletter",
                        )
                        continue
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è ÏûêÎèô ÏàòÏ†ï Ïã§Ìå®: {e}")

                draft.status = "needs_revision"
                if draft.fact_check_notes:
                    print(f"   ‚ö†Ô∏è ÏàòÏ†ï ÌïÑÏöî: {', '.join(draft.fact_check_notes[:2])}")
                else:
                    print(f"   ‚ö†Ô∏è ÏàòÏ†ï ÌïÑÏöî: Ìå©Ìä∏Ï≤¥ÌÅ¨ Ïã§Ìå®")
                break

            drafts.append(draft)
            if draft.status == "ready_for_review":
                ready_count += 1

            # Rate limit Î∞©ÏßÄ: ÎÖºÎ¨∏ ÏÇ¨Ïù¥ 10Ï¥à ÎåÄÍ∏∞
            if ready_count < target_ready:
                await asyncio.sleep(10)

        return drafts

    async def generate_single_content(
        self,
        paper: Paper,
        content_type: str = "newsletter"
    ) -> ContentDraft:
        """Îã®Ïùº ÎÖºÎ¨∏ ÏΩòÌÖêÏ∏† ÏÉùÏÑ±"""
        draft = await self.generator.generate_content(paper, content_type)
        fact_result = await self.fact_checker.check(draft)
        draft.fact_check_notes = fact_result.get("issues", [])
        draft.status = "ready_for_review" if fact_result.get("safe_to_publish") else "needs_revision"
        return draft

    def save_drafts(self, drafts: List[ContentDraft], output_dir: str = "content_drafts"):
        """Save drafts to JSON files"""
        os.makedirs(output_dir, exist_ok=True)

        for draft in drafts:
            filename = f"{draft.created_at[:10]}_{draft.content_type}_{draft.paper.doi.replace('/', '_')[:20]}.json"
            filepath = os.path.join(output_dir, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(asdict(draft), f, ensure_ascii=False, indent=2)

        print(f"\nüíæ {len(drafts)}Í∞ú ÏΩòÌÖêÏ∏† Ï†ÄÏû• ÏôÑÎ£å: {output_dir}/")


# Example usage
async def main():
    """Example: Run weekly content pipeline with multi-source discovery"""
    print("=" * 60)
    print("üß¨ Ïû•Ïàò ÏßÄÏãù ÌîåÎû´Ìèº - ÏûêÎèô ÏΩòÌÖêÏ∏† ÌååÏù¥ÌîÑÎùºÏù∏")
    print("=" * 60)

    # Source configuration
    include_preprints = os.getenv("INCLUDE_PREPRINTS", "true").lower() == "true"
    include_trials = os.getenv("INCLUDE_TRIALS", "true").lower() == "true"

    print("\nüì° Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§ ÏÑ§Ï†ï:")
    print(f"   ‚Ä¢ PubMed: ‚úÖ (Ìï≠ÏÉÅ ÌôúÏÑ±)")
    print(f"   ‚Ä¢ bioRxiv/medRxiv: {'‚úÖ' if include_preprints else '‚ùå'}")
    print(f"   ‚Ä¢ ClinicalTrials.gov: {'‚úÖ' if include_trials else '‚ùå'}")

    # Provider ÏÑ†ÌÉù (ÎπÑÏö© Ïàú: kimi < gemini < openai)
    ai_provider = os.getenv("AI_PROVIDER", "kimi")
    if not os.getenv("KIMI_API_KEY") and ai_provider == "kimi":
        ai_provider = "gemini"  # Fallback to gemini
    if not os.getenv("GEMINI_API_KEY") and ai_provider == "gemini":
        ai_provider = "openai"  # Fallback to openai

    # Fact-check provider should be stable and allowed to be different from generation.
    fact_check_provider = os.getenv("FACT_CHECK_PROVIDER", "").strip().lower() or ""
    if fact_check_provider not in ("kimi", "gemini", "openai"):
        fact_check_provider = ""
    if not fact_check_provider:
        # Prefer reliability for automation: OpenAI first (if configured),
        # then Kimi (cheap), then fall back to generation provider.
        if os.getenv("OPENAI_API_KEY"):
            fact_check_provider = "openai"
        elif os.getenv("KIMI_API_KEY"):
            fact_check_provider = "kimi"
        else:
            fact_check_provider = ai_provider

    print(f"\nü§ñ AI Provider: {ai_provider} (fact-check: {fact_check_provider})")
    pipeline = ContentPipeline(ai_provider=ai_provider, fact_check_provider=fact_check_provider)

    # Run weekly pipeline with multi-source
    drafts = await pipeline.run_weekly_pipeline(
        include_preprints=include_preprints,
        include_trials=include_trials
    )

    # Save drafts
    pipeline.save_drafts(drafts)

    # Generate Instagram card news images
    try:
        from card_news_generator import CardNewsGenerator
        print("\nüé® Instagram Ïπ¥ÎìúÎâ¥Ïä§ ÏÉùÏÑ± Ï§ë...")
        card_gen = CardNewsGenerator()
        for draft in drafts:
            paths = card_gen.generate_for_draft(draft)
            print(f"   üì∏ {len(paths)} slides: {draft.korean_title[:40]}...")
        print("‚úÖ Ïπ¥ÎìúÎâ¥Ïä§ ÏÉùÏÑ± ÏôÑÎ£å")
    except Exception as e:
        print(f"‚ö†Ô∏è Ïπ¥ÎìúÎâ¥Ïä§ ÏÉùÏÑ± Ïã§Ìå® (Í≥ÑÏÜç ÏßÑÌñâ): {e}")

    # Instagram auto-posting (opt-in via env var)
    instagram_enabled = os.getenv("INSTAGRAM_AUTO_POST", "false").lower() == "true"
    if instagram_enabled:
        try:
            from instagram_poster import InstagramPoster
            print("\nüì± Instagram ÏûêÎèô Ìè¨Ïä§ÌåÖ Ï§ë...")
            poster = InstagramPoster(
                ig_user_id=os.getenv("INSTAGRAM_USER_ID"),
                access_token=os.getenv("INSTAGRAM_ACCESS_TOKEN"),
                github_repo=os.getenv("GITHUB_REPOSITORY", "chang-myungoh/longevity-platform"),
                pages_base_url=os.getenv("PAGES_BASE_URL", "https://chang-myungoh.github.io/longevity-platform"),
            )

            # Build list of (draft_data, card_news_dir) pairs
            drafts_with_dirs = []
            card_news_base = os.path.join(os.path.dirname(__file__), "card_news")
            for draft in drafts:
                from dataclasses import asdict as _asdict
                draft_data = _asdict(draft)
                # Reconstruct card_news dir name (matches CardNewsGenerator logic)
                date_str = draft.created_at[:10] if draft.created_at else datetime.now().strftime("%Y-%m-%d")
                doi = draft.paper.doi or ""
                slug = doi.replace("/", "-").replace(".", "-").lower()[:40] if doi else "untitled"
                card_dir = os.path.join(card_news_base, f"{date_str}_{slug}")
                if os.path.isdir(card_dir):
                    drafts_with_dirs.append((draft_data, card_dir))
                else:
                    print(f"   ‚ö†Ô∏è Card news not found: {card_dir}")

            await poster.post_batch(drafts_with_dirs)
        except Exception as e:
            print(f"‚ö†Ô∏è Instagram Ìè¨Ïä§ÌåÖ Ïã§Ìå® (Í≥ÑÏÜç ÏßÑÌñâ): {e}")

    # Summary with source info
    print("\n" + "=" * 60)
    print("üìä Í≤∞Í≥º ÏöîÏïΩ")
    print("=" * 60)
    for draft in drafts:
        status_icon = "‚úÖ" if draft.status == "ready_for_review" else "‚ö†Ô∏è"
        source_tag = ""
        if draft.paper.topics:
            if "clinical_trial" in draft.paper.topics:
                source_tag = "[ÏûÑÏÉÅÏãúÌóò] "
            elif "preprint" in draft.paper.topics:
                source_tag = "[ÌîÑÎ¶¨ÌîÑÎ¶∞Ìä∏] "

        print(f"{status_icon} {source_tag}{draft.korean_title}")
        print(f"   Ïã†Î¢∞ÎèÑ: {draft.confidence_score:.0%} | ÏÉÅÌÉú: {draft.status}")
        print(f"   Ï∂úÏ≤ò: {draft.paper.journal}")

    return drafts


async def demo_multi_source():
    """Demo: Test multi-source paper discovery without content generation"""
    print("=" * 60)
    print("üî¨ Î©ÄÌã∞ÏÜåÏä§ ÎÖºÎ¨∏ Í≤ÄÏÉâ Îç∞Î™®")
    print("=" * 60)

    discovery = PaperDiscovery()

    # Search specific topic across all sources
    query = "NAD+ longevity"
    print(f"\nüîç Í≤ÄÏÉâÏñ¥: {query}")

    results = await discovery.search_all_sources(
        query=query,
        max_per_source=5,
        days_back=30,
        include_trials=True
    )

    print(f"\nüìä Í≤ÄÏÉâ Í≤∞Í≥º:")
    for source, items in results.items():
        print(f"\n--- {source.upper()} ({len(items)}Í∞ú) ---")
        for item in items[:3]:  # Show top 3 per source
            if hasattr(item, 'title'):
                print(f"  ‚Ä¢ {item.title[:70]}...")
            elif hasattr(item, 'nct_id'):
                print(f"  ‚Ä¢ [{item.nct_id}] {item.title[:60]}...")


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "demo":
        asyncio.run(demo_multi_source())
    else:
        asyncio.run(main())
